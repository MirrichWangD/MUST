
    
<!DOCTYPE html>
<html lang="zh-hans">

<meta http-equiv="X-UA-Compatible" content="IE=7" />

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="author" content="Turnitin, LLC" />
    <meta name="keywords" content="" /> 
    <meta name="description" content="" />
<title>Turnitin - 原创性报告 - Research on Panoptic Segmentation Algorithm Based on Transformer </title>

<base href="http://www.turnitin.com">
<style type="text/css">
body {
    color: #333;
    background: #C0C7CC;
    padding: 0;
    border: 0;
    font: 13px Verdana, arial, sans-serif;
    margin: 0;
}

form {
    padding: 0;
    margin: 0;
}

body#display {
}

body#bodysource {
    width: 520px;
    background: #F0F4FA;
}

p {
    padding: 10px 18px;
    margin: 0;
}

img {
    border: 0;
    padding: 0;
}

div {
    padding: 0;
    border: 0;
}

iframe {
    border: 0;
    margin: 0;
    padding: 0;
}

strong {
    font-weight: bold;
}

ul {
    padding: 0;
    margin: 0;
    list-style-type: none;
    font-size: 13px;
}

ul li {
    padding: 0;
    margin: 0;
    line-height: 16px;
}

#index span#exclude {
    margin: 0 50px 0 23px;
}

#index a {
    font-size: 11px;
    padding: 0 8px;
}

#index select {
    font-size: 12px;
    border: 1px solid #888;
}

#index input.small {
    margin: 0 0 0 5px;
    width: 30px;
    color: #D10A0A;
    font-weight: bold;
    font-size: 13px;
    border: 1px solid #888;
    vertical-align: baseline;
}

div.links {
    width: 85%;
    margin: 0 auto;
    border-left: 1px solid #888;
    border-right: 1px solid #888;
    padding-top: 8px;
    background: #E8EEF7;
    text-align: left;
}

.links div {
    padding: 5px 13px 10px 20px;
    border-bottom: 1px dotted #888;
}

.links div p {
    padding: 2px 0 0 40px;
}

div#body {
    line-height: 17px;
    width: 85%;
    margin: auto;
    padding: 20px 0;
    background: #fff;
    border-bottom: 1px solid #888;
    border-right: 1px solid #888;
    border-left: 1px solid #888;
    text-align: left;
}

#body p {
    color: #000;
    padding-top: 10 0;
    margin: 0 40px;
}

#actions {
    display: none;
}

a.exclude {
    float: right;
    margin: 0;
    padding: 0;
    position: relative;
    bottom: 20px;
}

/*= SMALL MATCHES POPUP
=== === === === === === === === === === === === === === === === === === === === === === === === === === === === === === */
div#small_matches_prefs {
    visibility: hidden;
    position: absolute;
    top: 0;
    left: 400px;
    background-color: #FFF;
    border: 1px solid #999;
    text-align: right;
}

div#small_matches_prefs p {
    padding: 7px;
}

div#small_matches_prefs li {
    padding: 10px 40px 10px 0;
    border-bottom: 1px solid #999;
    cursor: pointer;
    text-align: left;
}

div#small_matches_prefs li.selected {
    background-color: #87A3C0;
}

div#small_matches_prefs li input {
    text-align: center;
    border: 1px solid #999;
}

div#small_matches_prefs li.disabled input {
    color: #878787;
    background-color: #E6E5E6;
}

div#small_matches_prefs ul label {
    width: 100px;
    text-align: right;
    display: inline-block;
    margin-left: 10px;
    margin-right: 10px;
}
/*= GENERAL 
=== === === === === === === === === === === === === === === === === === === === === === === === === === === === === === */

body #top_bar {
    display: none !important;
}

body #index #exclude,
#download_button,
#print_button,
#index .right {
    display: inline-block;
}

body #index {
    width: 85%;
    margin-left: auto;
    margin-right: auto;
    border: 1px solid #999;
    background: #ececec url(new_dynamic/images/22bd7a01a025b8de122259e42762f0a7cb_ug_toolbar_bg.gif) repeat-x center left;
}

#toolbar_wrapper {
    padding-left: 45px;
}

body #top {
    width: 85%;
    background-color: #FFF;
    margin-left: auto;
    margin-right: auto;
    border: 1px solid #999;
    border-bottom: none;
    height: 210px;
}

body #content {
    padding: 10px 60px;
}

body div#prefs {
    display: none;
}

body #top h1 {
    font-size: 20px;
    font-weight: normal;
}

body #top h1 strong {
    font-weight: normal;
}

body #top h1 em {
    font-style: normal;
}


/*body #top h2 {*/
/*    font-size: 20px;*/
/*    font-weight: normal;*/
/*}*/

/*body #top h2 strong {*/
/*    font-weight: normal;*/
/*}*/

/*body #top h2 em {*/
/*    font-style: normal;*/
/*}*/


body #top h2 {
    font-size: 16px;
    font-weight: normal;
}

body #top h2 strong {
    font-weight: normal;
}

body #top h2 em {
    font-style: normal;
}


body #top_body li { /*Paper info li*/
    padding: 0;
    margin: 0px 0px 2px 0px;
    font-size: 10px;
}

#top_body #print_wrapper {
    float: left;
    width: 50%;
}

#top_body .similarity_print_wrapper {
    width: 45%;
    min-width: 283px;
}

#top_body .similarity_box { /*Similarity Box w/ Similarity by Source */
    float: right;
    border: solid 1px #666;
    margin-top: 60px;
    width: 350px;
}

#top_body .similarity_box .overall_similarity {
    float: left;
    border-right: solid 1px #666;
}

#top_body .similarity_box .overall_similarity .color_box {
    font-size: 14px;
    width: 140px;
}

#top_body .color_box.green {
    background-color: green;
}

#top_body .color_box.blue {
    background-color: blue;
}

#top_body .color_box.yellow {
    background-color: yellow;
}

#top_body .color_box.orange {
    background-color: orange;
}

#top_body .color_box.red {
    background-color: red;
}

#top_body .similarity_box .overall_similarity .similarity_title {
    font-size: 13px;
    font-weight: normal;
    padding: 5px 0px 0px;
    text-align: center;
}

#top_body .similarity_box .overall_similarity .similarity_percent {
    font-size: 25px;
    font-family: georgia, times, serif;
    padding: 5px 0px 15px;
    text-align: center;
}

#top_body .similarity_box .overall_similarity a {
    display: none;
}

#top_body .similarity_box .similarity_by_source {
    float: right;
    font-size: 10px;
}

#top_body .similarity_box .similarity_by_source .similarity_title {
    padding: 6px 0px 0px 10px;
    font-weight: bold;
    text-align: left;
}

#top_body .similarity_box .similarity_by_source dl {
    padding-left: 10px;
    margin: 14px 7px 0px 0px;
}

#top_body .similarity_box .similarity_by_source dt {
    float: left;
    width: 160px;
}

#top_body .similarity_box .similarity_by_source dd {
    float: left;
    margin: 0px;
}


#index span#exclude {
    margin: 0 50px 0 23px;
}

#index a {
    font-size: 11px;
    padding: 0 8px;
}

#index select {
    font-size: 12px;
    border: 1px solid #888;
}

#index input.small {
    margin: 0 0 0 5px;
    width: 30px;
    color: #D10A0A;
    font-weight: bold;
    font-size: 13px;
    border: 1px solid #888;
    vertical-align: baseline;
}

div.links {
    width: 85%;
    margin: 0 auto;
    border-left: 1px solid #888;
    border-right: 1px solid #888;
    padding-top: 8px;
    background: #E8EEF7;
    text-align: left;
}

.links div {
    padding: 5px 13px 10px 20px;
    border-bottom: 1px dotted #888;
}

.links div p {
    padding: 2px 0 0 40px;
}

div#body {
    line-height: 17px;
    width: 85%;
    margin: auto;
    padding: 20px 0;
    background: #fff;
    border-bottom: 1px solid #888;
    border-right: 1px solid #888;
    border-left: 1px solid #888;
    text-align: left;
}

#body p {
    color: #000;
    padding-top: 10 0;
    margin: 0 40px;
}

#actions {
    display: none;
}

button.exclude {
    float: right;
    margin: 0;
    padding: 0;
    border: none;
}

#small_matches_prefs {
    display: none;
}

/*
Copyright (c) 2009, Yahoo! Inc. All rights reserved.
Code licensed under the BSD License:
http://developer.yahoo.net/yui/license.txt
version: 2.7.0
*/
.yui-button{display:-moz-inline-box;display:inline-block;vertical-align:text-bottom;}.yui-button .first-child{display:block;*display:inline-block;}.yui-button button,.yui-button a{display:block;*display:inline-block;border:none;margin:0;}.yui-button button{background-color:transparent;*overflow:visible;cursor:pointer;}.yui-button a{text-decoration:none;}.yui-skin-sam .yui-button{border-width:1px 0;border-style:solid;border-color:#808080;background:url(../images/yui270/build/assets/skins/sam/96b257a32a932f7739d7dab52b38ee8fcb_sprite.png) repeat-x 0 0;margin:auto .25em;}.yui-skin-sam .yui-button .first-child{border-width:0 1px;border-style:solid;border-color:#808080;margin:0 -1px;_margin:0;}.yui-skin-sam .yui-button button,.yui-skin-sam .yui-button a{padding:0 10px;font-size:93%;line-height:2;*line-height:1.7;min-height:2em;*min-height:auto;color:#000;}.yui-skin-sam .yui-button a{*line-height:1.875;*padding-bottom:1px;}.yui-skin-sam .yui-split-button button,.yui-skin-sam .yui-menu-button button{padding-right:20px;background-position:right center;background-repeat:no-repeat;}.yui-skin-sam .yui-menu-button button{background-image:url(yui270/build/button/assets/skins/sam/6305efb37fa05af65c79b58b9d4c1b03cb_menu-button-arrow.png);}.yui-skin-sam .yui-split-button button{background-image:url(yui270/build/button/assets/skins/sam/ced974d5c685e5dfa0a37b824a6b5d48cb_split-button-arrow.png);}.yui-skin-sam .yui-button-focus{border-color:#7D98B8;background-position:0 -1300px;}.yui-skin-sam .yui-button-focus .first-child{border-color:#7D98B8;}.yui-skin-sam .yui-button-focus button,.yui-skin-sam .yui-button-focus a{color:#000;}.yui-skin-sam .yui-split-button-focus button{background-image:url(yui270/build/button/assets/skins/sam/36e66540d2feba76b8991e18b76fe93bcb_split-button-arrow-focus.png);}.yui-skin-sam .yui-button-hover{border-color:#7D98B8;background-position:0 -1300px;}.yui-skin-sam .yui-button-hover .first-child{border-color:#7D98B8;}.yui-skin-sam .yui-button-hover button,.yui-skin-sam .yui-button-hover a{color:#000;}.yui-skin-sam .yui-split-button-hover button{background-image:url(yui270/build/button/assets/skins/sam/36e66540d2feba76b8991e18b76fe93bcb_split-button-arrow-hover.png);}.yui-skin-sam .yui-button-active{border-color:#7D98B8;background-position:0 -1700px;}.yui-skin-sam .yui-button-active .first-child{border-color:#7D98B8;}.yui-skin-sam .yui-button-active button,.yui-skin-sam .yui-button-active a{color:#000;}.yui-skin-sam .yui-split-button-activeoption{border-color:#808080;background-position:0 0;}.yui-skin-sam .yui-split-button-activeoption .first-child{border-color:#808080;}.yui-skin-sam .yui-split-button-activeoption button{background-image:url(yui270/build/button/assets/skins/sam/890272b241c1d8a0db3ce5680b71fab0cb_split-button-arrow-active.png);}.yui-skin-sam .yui-radio-button-checked,.yui-skin-sam .yui-checkbox-button-checked{border-color:#304369;background-position:0 -1400px;}.yui-skin-sam .yui-radio-button-checked .first-child,.yui-skin-sam .yui-checkbox-button-checked .first-child{border-color:#304369;}.yui-skin-sam .yui-radio-button-checked button,.yui-skin-sam .yui-checkbox-button-checked button{color:#fff;}.yui-skin-sam .yui-button-disabled{border-color:#ccc;background-position:0 -1500px;}.yui-skin-sam .yui-button-disabled .first-child{border-color:#ccc;}.yui-skin-sam .yui-button-disabled button,.yui-skin-sam .yui-button-disabled a{color:#A6A6A6;cursor:default;}.yui-skin-sam .yui-menu-button-disabled button{background-image:url(yui270/build/button/assets/skins/sam/4df7235ca027f2546b2a216e59f81fb0cb_menu-button-arrow-disabled.png);}.yui-skin-sam .yui-split-button-disabled button{background-image:url(yui270/build/button/assets/skins/sam/db73dce6da2f5c5f02399c93488ce69ecb_split-button-arrow-disabled.png);}

</style>



</head>

<body onload="">



<link rel="stylesheet" type="text/css" href="/r/build/css/tii/88ee4ccd3555f2b759921fb5d58d83e5cb_container.css" media="all" />




<script type="text/javascript" src="/r/build/js/tii/8b608684a5f4aec1b540987c93498c01cb_tii_anonymous_marking.js"></script>




<script type="text/javascript">

function initAnonymousMarking () {
    // initialize panel.  
    var config = {
            zindex: 4,
            underlay: 'none',
            modal: true,
            visible: false,
            draggable: false,
            close: false,
            fixedcenter: true
    };
    if ($('disable_anonymous_marking')) {
        disableAnonymousMarkingPanel = new IP.widget.Panel($('disable_anonymous_marking'), config);
        if($D.hasClass('disable_anonymous_marking', 'app')) {
            disableAnonymousMarkingPanel.center = function () {
                var nViewportOffset = 20,
                    elementWidth = this.element.offsetWidth,
                    elementHeight = this.element.offsetHeight,
                    viewPortWidth = $D.getViewportWidth(),
                    viewPortHeight = $D.getViewportHeight(),
                    x,
                    y;

                if (elementWidth < viewPortWidth) {
                    x = (viewPortWidth / 2) - (elementWidth / 2) + $D.getDocumentScrollLeft();
                } else {
                    x = nViewportOffset + $D.getDocumentScrollLeft();
                }

				if (browser == 'Internet Explorer') {
					x = 0;
				}
                y = 2 + $D.getDocumentScrollTop();

                this.cfg.setProperty("xy", [parseInt(x, 10), parseInt(y, 10)]);
                this.cfg.refireEvent("iframe");
            };
        }
        disableAnonymousMarkingPanel.render(document.body);
        disableAnonymousMarkingPanel.hideEvent.subscribe(function () { $('anonymous_error').innerHTML = ''; }, false);
        disableAnonymousMarkingPanel.hide();
        Element.show($('disable_anonymous_marking'));
    }
}

function disableAM (data) {
    $('anonymous_title').innerHTML = data.title;
    document.disable_anonymous_marking_form.objectid.value = data.oid;
    disableAnonymousMarkingPanel.show();
}

function checkDisableAM () {
    var form = document.disable_anonymous_marking_form;
    if (form.reason.value.length <= 5) {
        $('anonymous_error').innerHTML = "请提供关闭匿名标记的原因。您的理由必须超过 10 个字符长度。";
    }
    else {
        form.submit();
    }
    return;
}

YAHOO.util.Event.onDOMReady(initAnonymousMarking);

</script>

<div id="disable_anonymous_marking" class="app" style="display: none;">
<div class="anonymous_frames">
<form method="post" name="disable_anonymous_marking_form">
    <input type="hidden" name="objectid" value=""/>
    <input type="hidden" name="disable_anonymous_marking" value="1"/> 
    <div class="anonymous_header">
    	<h1>关闭匿名标记</h1>
		<p>请说明关闭匿名标记的原因： <span id="anonymous_title"></span><br />
			<strong>警告: 管理员可以存取此资讯。此设定是永久性的。</strong>
        
            </p>
        
    </div>
    <div class="anonymous_body">
        <textarea name="reason" cols="30" rows="3"></textarea>
        <p id="anonymous_error"></p>
    </div>
    <div class="anonymous_footer">
		<div class="anonymous_footer_buttons">
            <span class="submit_form_button"><input type="button" onClick="checkDisableAM();" value="提交"></span><br>
        	<span class="submit_form_button"><input type="button" value="取消" onClick="disableAnonymousMarkingPanel.hide();"></span>
		</div>
    </div>
</form>
</div>
</div>
<div id="actions">
<p>这是您报告的印表版的检视。请点击 "打印" 以继续或 "结束" 以关闭视窗。</p>
<script type="text/javascript" language="javascript">
	var browserName=navigator.appName;
	var browserVer=parseInt(navigator.appVersion);
	if ((navigator.appVersion.indexOf("Mac")!=-1) && (browserName == "Microsoft Internet Explorer")) {
		document.write('<span class="AR10">键入 COMMAND-P 开始打印。</span><br><br>');
	} else {
		document.write('<button onclick="window.print();">打印</button>&nbsp;&nbsp;');
	}
</script>
<button onclick="window.close();">完成</button>
</div>


<!-- ########################### Preferences pop-up ##########################--> 

<div name="top" id="header">

<div id="prefs" role="dialog" style="display:none" aria-labelledby="prefs_link" aria-describedby="prefs_link" aria-owns="prefs_link">
<div class="padding">
<form name="prefs_form" method="post" accept-charset="utf-8">
<script type="text/javascript" language="javascript">
function savePrefs(){
	if (document.prefs_form.changed.value == 1){
		document.prefs_form.submit();
	}else{
        hidePrefsPane();
	}
}

function handlePrefsPaneKeyUp (evt) {
    // first check for IME compositions and ignore
    if (evt.isComposing || evt.keyCode === 229) {
        return;
    }
    if (evt.key === "Escape") {
        evt.preventDefault();
        evt.stopPropagation();
        hidePrefsPane();
    }
}

function showPrefsPane(){
    const prefsDiv = document.getElementById('prefs');
    prefsDiv.style.display='block';
    prefsDiv.addEventListener('keyup', handlePrefsPaneKeyUp);
    document.getElementById('use_colors').focus();
}

function hidePrefsPane(){
    const prefsDiv = document.getElementById('prefs');
    prefsDiv.style.display='none';
    prefsDiv.removeEventListener('keyup', handlePrefsPaneKeyUp);
    document.getElementById('prefs_link').focus();
}

var overlay, $D, $;

function handleSmallMatchesPrefKeyUp (evt) {
    // first check for IME compositions and ignore
    if (evt.isComposing || evt.keyCode === 229) {
        return;
    }
    if (evt.key === "Escape") {
        evt.preventDefault();
        evt.stopPropagation();
        hideSmallMatchExclusions();
    }
}

function showSmallMatchExclusions(left) {
    $D = YAHOO.util.Dom;
    $E = YAHOO.util.Event;
    $ = $D.get;

    $D.setStyle('small_matches_prefs', 'top', $D.getDocumentScrollTop() + 187 + 'px');
    $D.setStyle('small_matches_prefs', 'left', left + 'px');

    $D.setStyle('small_matches_prefs', 'display', 'block');
    $D.setStyle('small_matches_prefs', 'visibility', 'visible');

        // focus the field
    $D.hasClass('exclude_by_percent_row', 'selected') ? $('exclude_by_percent_value').focus() : $('exclude_by_words_value').focus();
    $E.on(window, 'scroll', repositionDialog);



/*
A problem occurs: when the focus moves from the wordcount field to the percentage field
the existing percentage is floored. So even if the value is "correct", it gets incorrect,
because the floored percentage is different from the word count. Especially in bigger texts and smaller matches
this becomes an issue. So, the percentage displayed when updated from the word count should be the rounded one.
only when the input itself is given manually, should this override the exclusionPercent value.

This can be done as long as we use the functions below as actual keyboard handlers, so we can filter between numerical
values entered and other keys (such as tab). Moreover, we could do up and down to increase or decrease the value.

Because we will need to keep a state of the unrounded percentage, it is better to have that percentage value
closed over.

*/

    let rawPercentage = 0;
    function updateExcludePercentage(evt) {
        // prevent symbol composing to interfere, 229 is a special code for the composition key
        if (evt.isComposing || evt.keyCode === 229) {
            return;
        }
        if (evt.key === "Escape") {
            evt.preventDefault();
            evt.stopPropagation();
            hideSmallMatchExclusions();
        }
        else if (/[0-9]/.exec(evt.key) || evt.key === 'Backspace' || evt.key === 'Delete') { //only recalculate when the entered keys represent numbers
            // only when entering or changing the value in the input field, it is clear that the user
            // intended to use this specific exclusion method
            selectSmallExclusionMethod('words');
            const wordCount = this.value;

            rawPercentage = wordCount / 5610 * 100;
            $('exclude_by_percent_value').value = Math.floor(rawPercentage); // only display floored value
        }
    }

    function updateExcludeWordCount(evt) {
        // prevent symbol composing to interfere, 229 is a special code for the composition key
        if (evt.isComposing || evt.keyCode === 229) {
            return;
        }
        if (evt.key === "Escape") {
            evt.preventDefault();
            evt.stopPropagation();
            hideSmallMatchExclusions();
        }
        else if (/[0-9]/.exec(evt.key) || evt.key === 'Backspace' || evt.key === 'Delete') { //only recalculate when the entered keys represent numbers
            // only when entering or changing the value in the input field, it is clear that the user
            // intended to use this specific exclusion method
            selectSmallExclusionMethod('percent');

            const percent = this.value;
            var wordCount = Math.floor((percent/100) * 5610);

            $('exclude_by_words_value').value = wordCount;
        }
    }
    // set to global name space so hideSmallMatchesExclusions can also remove the listeners.
    if (!window.updateExcludePercentage) window.updateExcludePercentage = updateExcludePercentage;
    if (!window.updateExcludeWordCount) window.updateExcludeWordCount = updateExcludeWordCount;

    document.getElementById('small_matches_prefs').addEventListener('keyup', handleSmallMatchesPrefKeyUp);
    document.getElementById('exclude_by_words_value').addEventListener('keyup', updateExcludePercentage);
    document.getElementById('exclude_by_percent_value').addEventListener('keyup', updateExcludeWordCount);

}

function repositionDialog() {
    $D.setStyle('small_matches_prefs', 'top', $D.getDocumentScrollTop() + 187 + 'px');
}

function hideSmallMatchExclusions() {
    document.getElementById('small_matches_prefs').removeEventListener('keyup', handleSmallMatchesPrefKeyUp);
    document.getElementById('exclude_by_words_value').removeEventListener('keyup', updateExcludePercentage);
    document.getElementById('exclude_by_percent_value').removeEventListener('keyup', updateExcludeWordCount);

    $D.setStyle('small_matches_prefs', 'display', 'none');
    $E.removeListener(window, 'scroll', repositionDialog);
    document.getElementById('exclude_small_matches_link').focus();
}

function selectSmallExclusionMethod(enableType) {
    if(enableType == 'words') {
        $('exclude_by_words_value').focus();
        
        $D.addClass('exclude_by_words_row', 'selected');
        $D.removeClass('exclude_by_percent_row', 'selected');
        $D.removeClass('exclude_by_words_row', 'disabled');
        $D.addClass('exclude_by_percent_row', 'disabled');
    }
    else {
        $('exclude_by_percent_value').focus();
        
        $D.removeClass('exclude_by_words_row', 'selected');
        $D.addClass('exclude_by_percent_row', 'selected');
        $D.addClass('exclude_by_words_row', 'disabled');
        $D.removeClass('exclude_by_percent_row', 'disabled');
    }
}

function submitSmallMatchesChange() {
    var excludeBy = $D.hasClass('exclude_by_percent_row', 'selected') ? 'percent' : 'words';
    var excludeValue = excludeBy == 'percent' ? $('exclude_by_percent_value').value : $('exclude_by_words_value').value;
    
    changeSmallMatchExclusion(excludeBy, parseInt(excludeValue), 5610);
}


</script>
<input type="hidden" name="changed" value="0">
        <div class="pref_rows">
                <label for="use_colors">颜色代码匹配：</label>
                <select id="use_colors" name="use_colors" onchange="document.prefs_form.changed.value=1">
                    <option value="1">是
                    <option value="0">否 
                </select>
                <div class="clear"></div>
        </div>
        <div class="pref_rows">
                <label for="def_report_mode">预设模式：</label>
                <select id="def_report_mode" name="def_report_mode" onchange="document.prefs_form.changed.value=1">
                    <option value="0">显示所有匹配度最高的
                    <option value="1">每次显示一个匹配的
                    <option value="2">快速查看报告（经典页面）
                </select>
                <div class="clear"></div>
        </div>
        <div class="pref_rows">
                <label for="report_scrolling">自动导航</label>
                <select id="report_scrolling" name="report_scrolling" onchange="document.prefs_form.changed.value=1">
                    <option value="0">跳到下一个符合处
                    <option value="1">移动到下一个谋和处
                </select>
                <div class="clear"></div>
        </div>
        
        <div id="prefs_confirm">
            <button onClick="savePrefs()" >储存</button>  
            <button onClick="hidePrefsPane();">取消</button>
        </div>
</form>
</div>
</div>
</div>
<!-- ########################### END Preferences pop-up  ##########################--> 

<!-- ########################### BEGIN small matches pop-up  ##########################--> 
<div id="small_matches_prefs" role="dialog" aria-labelledby="exclude_small_matches_link" aria-describedby="exclude_small_matches_link" aria-owns="exclude_small_matches_link">
    <form onsubmit="submitSmallMatchesChange(); return false;">
        <ul>
            <li id="exclude_by_words_row" class="selected">
                <label for="exclude_by_words_value">字数: </label>
                <input type="text" id="exclude_by_words_value" size="3" value="" onkeyup="updateExcludePercentage"> 字
            </li>
            <li id="exclude_by_percent_row" class="disabled">
                <label for="exclude_by_percent_value">百分比: </label>
                <input type="text" id="exclude_by_percent_value" size="3" value="" max-length="3" onkeyup="updateExcludeWordCount"> %
            </li>
        </ul>
        <p><input type='submit' value="提交"> 或 <button onclick="hideSmallMatchExclusions()">取消</button></p>
    </form>
</div>

<!-- ########################### END small matches pop-up  ##########################-->

<!-- ########################### Top of Report  ##########################--> 
<div id="top">
    <div id="content" role="banner">
    
        <!-- ######### Top Bar  ##########################--> 
        <div id="top_bar">
                <ul id="top_bar_list1">
                      <!-- Preferences --><li><button id="prefs_link" onclick="showPrefsPane(); aria-haspopup="dialog">preferences</button></li>
                </ul>
                <ul id="top_bar_list2">
                      
                </ul>
                <div class="clear"></div>
        </div>   
        <!-- ######### END Top Bar  ##########################--> 
        
        
        <!-- ######### Top Body  ##########################--> 
        <div id="top_body">
        
            <div id="print_wrapper">
                <div class="general_info" role="region" aria-label="文稿资讯">
                    <!-- Logo --> 
                    <h1>
                        <span class=""></span>
                        <strong>Turnitin</strong>
                        <em>原创性报告</em>
                     </h1>
                 
                     <!-- Paper Info -->               
                     <ul>
                         <li>已处理到: 2023年12月27日  1:41 上午 PST</li>
                         <li>代码: 2265065293 </li>
                         <li>字数: 5610</li>
                         <li>已提交: 1</li>
                     </ul>
                </div>

                 <!-- Paper Title --> 
                <h2>
                    <strong>Research on Panoptic Segmentation Algorithm Based on Transformer</strong> 
                    
                    <em>整合者 SIDC42d2258m8</em>
                    
                </h2>
            </div>
            
            <div id="similarity_print_wrapper">
                <div class="similarity_box" role="region" aria-labelledby="similarity_index_title" aria-describedby="similarity_index_title">
                    <div class="overall_similarity">
                        <div class="color_box green">&nbsp;</div>
                        <div id="similarity_index_title" class="similarity_title">相似度指标</div>
                        <div class="similarity_percent">13%</div>
                    </div>
                    <div class="similarity_by_source" role="region" aria-labelledby="similarity_by_source_title" aria-describedby="similarity_by_source_title">
                        <div id="similarity_by_source_title" class="similarity_title">依來源标示相似度</div>
                        <dl>
                            <dt>Internet&nbsp;Sources:</dt>
                            <dd>6%</dd>
                            <div class="clear"></div>
                            <dt>出版物:</dt>
                            <dd>11%</dd>
                            <div class="clear"></div>
                            <dt>学生文稿:</dt>
                            <dd>1%</dd>
                            <div class="clear"></div>
                        </dl>
                    </div>
                </div>
            </div>
            <div class="clear"></div>
                                 
        </div>
        <!-- ######### END Top Body  ##########################--> 
        
        
    </div>
</div>
<!-- ########################### END Top of Report  ##########################--> 



<!-- ########################### TOOLBAR  ##########################--> 
<div id="index">
	<div id="toolbar_wrapper" role="toolbar">
        
	</div>
</div>
<!-- ########################### END TOOLBAR  ##########################--> 


<div class="links" role="region" aria-label="相符总览">
    <div role="list">
	<div role="listitem" aria-setsize="65" aria-posinset="1">
	    <p>
	        1% match (Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(742805768,37,'0')" target="_blank" style="color:#D10A0A">Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="2">
	    <p>
	        1% match (从 2022年05月10日 来的学生文稿)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(2313591160,1,'0')" target="_blank" style="color:#287B28">Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="3">
	    <p>
	        < 1% match (从 2021年06月12日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.hindawi.com/journals/js/2021/6659831/" target="_blank" style="color:blue">https://www.hindawi.com/journals/js/2021/6659831/</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="4">
	    <p>
	        < 1% match (从 2023年10月16日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.hindawi.com/journals/ijis/2023/8879622/" target="_blank" style="color:brown">https://www.hindawi.com/journals/ijis/2023/8879622/</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="5">
	    <p>
	        < 1% match (从 2022年10月17日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.hindawi.com/journals/cmmm/2022/7852958/" target="_blank" style="color:#B64B01">https://www.hindawi.com/journals/cmmm/2022/7852958/</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="6">
	    <p>
	        < 1% match (从 2023年04月22日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.hindawi.com/journals/ijis/2023/6266209/" target="_blank" style="color:#630000">https://www.hindawi.com/journals/ijis/2023/6266209/</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="7">
	    <p>
	        < 1% match (从 2020年09月19日 的网络)
	    </p>
        
        
 
	    <p><a href="http://export.arxiv.org/pdf/2003.13198" target="_blank" style="color:#0270B6">http://export.arxiv.org/pdf/2003.13198</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="8">
	    <p>
	        < 1% match (从 2023年04月20日 的网络)
	    </p>
        
        
 
	    <p><a href="http://export.arxiv.org/pdf/2303.07011" target="_blank" style="color:#330099">http://export.arxiv.org/pdf/2303.07011</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="9">
	    <p>
	        < 1% match (从 2023年12月01日 的网络)
	    </p>
        
        
 
	    <p><a href="https://export.arxiv.org/pdf/2210.05958" target="_blank" style="color:#227967">https://export.arxiv.org/pdf/2210.05958</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="10">
	    <p>
	        < 1% match ("Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(775746707,37,'0')" target="_blank" style="color:#CB0099">"Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="11">
	    <p>
	        < 1% match ()
	    </p>
        
        
 
	    <p><a href="http://arxiv.org/abs/2011.09094" target="_blank" style="color:#006331">Dai, Zhigang, Cai, Bolun, Lin, Yugeng, Chen, Junying. "UP-DETR: Unsupervised Pre-training for Object Detection with  Transformers", 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="12">
	    <p>
	        < 1% match ()
	    </p>
        
        
 
	    <p><a href="http://arxiv.org/abs/2102.08009" target="_blank" style="color:#795AB9">Sirohi, Kshitij, Mohan, Rohit, Büscher, Daniel, Burgard, Wolfram, Valada, Abhinav. "EfficientLPS: Efficient LiDAR Panoptic Segmentation", 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="13">
	    <p>
	        < 1% match ()
	    </p>
        
        
 
	    <p><a href="http://arxiv.org/abs/2309.01823" target="_blank" style="color:#935F32">Pan, Shaoyan, Liu, Yiqiao et al. "Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in  Multiple Anatomical Locations", 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="14">
	    <p>
	        < 1% match ("Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(750670572,37,'0')" target="_blank" style="color:#ce0031">"Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="15">
	    <p>
	        < 1% match ("Computer Vision – ECCV 2020", Springer Science and Business Media LLC, 2020)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(671391770,37,'0')" target="_blank" style="color:#866712">"Computer Vision – ECCV 2020", Springer Science and Business Media LLC, 2020</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="16">
	    <p>
	        < 1% match ("Neural Information Processing", Springer Science and Business Media LLC, 2017)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(584318268,37,'0')" target="_blank" style="color:#63009c">"Neural Information Processing", Springer Science and Business Media LLC, 2017</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="17">
	    <p>
	        < 1% match (Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(811955287,37,'0')" target="_blank" style="color:#A85503">Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="18">
	    <p>
	        < 1% match (从 2022年10月01日 的网络)
	    </p>
        
        
 
	    <p><a href="https://web.archive.org/web/20220810064324if_/https:/arxiv.org/pdf/2208.00662v2.pdf" target="_blank" style="color:#cc0066">https://web.archive.org/web/20220810064324if_/https:/arxiv.org/pdf/2208.00662v2.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="19">
	    <p>
	        < 1% match (从 2022年10月01日 的网络)
	    </p>
        
        
 
	    <p><a href="https://web.archive.org/web/20210410111526if_/https:/arxiv.org/pdf/2103.14969v1.pdf" target="_blank" style="color:#21785B">https://web.archive.org/web/20210410111526if_/https:/arxiv.org/pdf/2103.14969v1.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="20">
	    <p>
	        < 1% match (从 2023年09月17日 的网络)
	    </p>
        
        
 
	    <p><a href="https://ebin.pub/computer-vision-eccv-2022-workshops-tel-aviv-israel-october-2327-2022-proceedings-part-i-3031250559-9783031250552.html" target="_blank" style="color:#336699">https://ebin.pub/computer-vision-eccv-2022-workshops-tel-aviv-israel-october-2327-2022-proceedings-part-i-3031250559-9783031250552.html</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="21">
	    <p>
	        < 1% match (从 2023年06月11日 的网络)
	    </p>
        
        
 
	    <p><a href="https://ebin.pub/urban-intelligence-and-applications-proceedings-of-icuia-2019-1st-ed-9783030450984-9783030450991.html" target="_blank" style="color:#D10A0A">https://ebin.pub/urban-intelligence-and-applications-proceedings-of-icuia-2019-1st-ed-9783030450984-9783030450991.html</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="22">
	    <p>
	        < 1% match (从 2023年07月29日 的网络)
	    </p>
        
        
 
	    <p><a href="https://d-nb.info/129739903X/34" target="_blank" style="color:#287B28">https://d-nb.info/129739903X/34</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="23">
	    <p>
	        < 1% match (从 2019年08月09日 的网络)
	    </p>
        
        
 
	    <p><a href="https://d-nb.info/119244177X/34" target="_blank" style="color:blue">https://d-nb.info/119244177X/34</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="24">
	    <p>
	        < 1% match ("Image and Graphics", Springer Science and Business Media LLC, 2017)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(584416229,37,'0')" target="_blank" style="color:brown">"Image and Graphics", Springer Science and Business Media LLC, 2017</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="25">
	    <p>
	        < 1% match ("Pattern Recognition and Computer Vision", Springer Science and Business Media LLC, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(743244579,37,'0')" target="_blank" style="color:#B64B01">"Pattern Recognition and Computer Vision", Springer Science and Business Media LLC, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="26">
	    <p>
	        < 1% match (Xiaole Fan, Xiufang Feng. "SELDNet: Sequenced encoder and lightweight decoder network for COVID-19 infection region segmentation", Displays, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(789843484,37,'0')" target="_blank" style="color:#630000">Xiaole Fan, Xiufang Feng. "SELDNet: Sequenced encoder and lightweight decoder network for COVID-19 infection region segmentation", Displays, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="27">
	    <p>
	        < 1% match (Diego A. Velazquez, Josep M. Gonfaus, Pau Rodriguez, F. Xavier Roca, Seiichi Ozawa, Jordi Gonzalez. "Logo Detection with No Priors", IEEE Access, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(735514980,37,'0')" target="_blank" style="color:#0270B6">Diego A. Velazquez, Josep M. Gonfaus, Pau Rodriguez, F. Xavier Roca, Seiichi Ozawa, Jordi Gonzalez. "Logo Detection with No Priors", IEEE Access, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="28">
	    <p>
	        < 1% match (Wouter A. J. Van den Broeck, Toon Goedemé. "Combining Deep Semantic Edge and Object Segmentation for Large-Scale Roof-Part Polygon Extraction 从 Ultrahigh-Resolution Aerial Imagery", Remote Sensing, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(783677469,37,'0')" target="_blank" style="color:#330099">Wouter A. J. Van den Broeck, Toon Goedemé. "Combining Deep Semantic Edge and Object Segmentation for Large-Scale Roof-Part Polygon Extraction from Ultrahigh-Resolution Aerial Imagery", Remote Sensing, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="29">
	    <p>
	        < 1% match (从 2022年02月10日 的网络)
	    </p>
        
        
 
	    <p><a href="https://link.springer.com/article/10.1007/s11042-021-11138-x?code=d5f07e4d-17f2-430e-a4a3-0934df95263a&error=cookies_not_supported" target="_blank" style="color:#227967">https://link.springer.com/article/10.1007/s11042-021-11138-x?code=d5f07e4d-17f2-430e-a4a3-0934df95263a&error=cookies_not_supported</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="30">
	    <p>
	        < 1% match (Gang Zhang, Chenhong Zheng, Jianfeng He, Sanli Yi. "PCT: Pyramid convolutional transformer for parotid gland tumor segmentation in ultrasound images", Biomedical Signal Processing and Control, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(788461632,37,'0')" target="_blank" style="color:#CB0099">Gang Zhang, Chenhong Zheng, Jianfeng He, Sanli Yi. "PCT: Pyramid convolutional transformer for parotid gland tumor segmentation in ultrasound images", Biomedical Signal Processing and Control, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="31">
	    <p>
	        < 1% match (Mengchao Zhang, Yuan Zhang, Manshan Zhou, Kai Jiang, Hao Shi, Yan Yu, Nini Hao. "Application of Lightweight Convolutional Neural Network for Damage Detection of Conveyor Belt", Applied Sciences, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(737211230,37,'0')" target="_blank" style="color:#006331">Mengchao Zhang, Yuan Zhang, Manshan Zhou, Kai Jiang, Hao Shi, Yan Yu, Nini Hao. "Application of Lightweight Convolutional Neural Network for Damage Detection of Conveyor Belt", Applied Sciences, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="32">
	    <p>
	        < 1% match (Tao Zhang, Bo Jin, Wenjing Jia. "An anchor-free object detector based on soften optimized bi-directional FPN", Computer Vision and Image Understanding, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(791157473,37,'0')" target="_blank" style="color:#795AB9">Tao Zhang, Bo Jin, Wenjing Jia. "An anchor-free object detector based on soften optimized bi-directional FPN", Computer Vision and Image Understanding, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="33">
	    <p>
	        < 1% match (Xiaodong Liu, Haipeng Guo, Huanyu Liu, Junbao Li. "Domain migration representation learning for blind magnetic resonance image super-resolution", Biomedical Signal Processing and Control, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(803229940,37,'0')" target="_blank" style="color:#935F32">Xiaodong Liu, Haipeng Guo, Huanyu Liu, Junbao Li. "Domain migration representation learning for blind magnetic resonance image super-resolution", Biomedical Signal Processing and Control, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="34">
	    <p>
	        < 1% match (Yangdong Chen, Zhaolong Zhang, Yanfei Wang, Yuejie Zhang, Rui Feng, Tao Zhang, Weiguo Fan. "AE-Net: Fine-grained sketch-based image retrieval via attention-enhanced network", Pattern Recognition, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(768732947,37,'0')" target="_blank" style="color:#ce0031">Yangdong Chen, Zhaolong Zhang, Yanfei Wang, Yuejie Zhang, Rui Feng, Tao Zhang, Weiguo Fan. "AE-Net: Fine-grained sketch-based image retrieval via attention-enhanced network", Pattern Recognition, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="35">
	    <p>
	        < 1% match (从 2023年08月17日 的网络)
	    </p>
        
        
 
	    <p><a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/50373/Zhang_washington_0250E_25465.pdf?isAllowed=y&sequence=1" target="_blank" style="color:#866712">https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/50373/Zhang_washington_0250E_25465.pdf?isAllowed=y&sequence=1</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="36">
	    <p>
	        < 1% match ()
	    </p>
        
        
 
	    <p><a href="https://pure.hud.ac.uk/ws/files/14343760/Modelling_and_Simulation_of_Train_Brake_System_in_Low_Adhesion_Conditions.pdf" target="_blank" style="color:#63009c">Alturbeh, Hamid, Stow, Julian, Tucker, Gareth, Lawton, Alan. "Modelling and simulation of the train brake system in low adhesion conditions", 'SAGE Publications', 2020</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="37">
	    <p>
	        < 1% match (从 2023年10月17日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.mdpi.com/2072-666X/14/2/442" target="_blank" style="color:#A85503">https://WWW.MDPI.COM/2072-666X/14/2/442</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="38">
	    <p>
	        < 1% match ("Image and Video Technology", Springer Science and Business Media LLC, 2019)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(643682615,37,'0')" target="_blank" style="color:#cc0066">"Image and Video Technology", Springer Science and Business Media LLC, 2019</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="39">
	    <p>
	        < 1% match (Jie Li, Zuling Wang. "Visual Transformer for Image Splicing Localization", 2023 4th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE), 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(808219266,37,'0')" target="_blank" style="color:#21785B">Jie Li, Zuling Wang. "Visual Transformer for Image Splicing Localization", 2023 4th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE), 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="40">
	    <p>
	        < 1% match (Xiaolong Liu, Yuqing Hou, Anbang Yao, Yurong Chen, Keqiang Li. "CASNet: Common Attribute Support Network for image instance and panoptic segmentation", 2020 25th International Conference on Pattern Recognition (ICPR), 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(764360480,37,'0')" target="_blank" style="color:#336699">Xiaolong Liu, Yuqing Hou, Anbang Yao, Yurong Chen, Keqiang Li. "CASNet: Common Attribute Support Network for image instance and panoptic segmentation", 2020 25th International Conference on Pattern Recognition (ICPR), 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="41">
	    <p>
	        < 1% match (Yongfang Xie, Weitao Hu, Shiwen Xie, Lei He. "Surface Defect Detection Algorithm Based on Feature-Enhanced YOLO", Cognitive Computation, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(795080910,37,'0')" target="_blank" style="color:#D10A0A">Yongfang Xie, Weitao Hu, Shiwen Xie, Lei He. "Surface Defect Detection Algorithm Based on Feature-Enhanced YOLO", Cognitive Computation, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="42">
	    <p>
	        < 1% match (Zhihong Xu, Mingyang Fan, Yapeng Zhang. "Visual detection of eggs based on deep learning for egg picking robot", Journal of Physics: Conference Series, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(735063818,37,'0')" target="_blank" style="color:#287B28">Zhihong Xu, Mingyang Fan, Yapeng Zhang. "Visual detection of eggs based on deep learning for egg picking robot", Journal of Physics: Conference Series, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="43">
	    <p>
	        < 1% match (从 2023年09月30日 的网络)
	    </p>
        
        
 
	    <p><a href="https://dokumen.pub/progress-in-computer-recognition-systems-1st-ed-978-3-030-19737-7978-3-030-19738-4.html" target="_blank" style="color:blue">https://dokumen.pub/progress-in-computer-recognition-systems-1st-ed-978-3-030-19737-7978-3-030-19738-4.html</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="44">
	    <p>
	        < 1% match ()
	    </p>
        
        
 
	    <p><a href="http://hdl.handle.net/10251/144574" target="_blank" style="color:brown">Samaniego-Riera, Franklin Eduardo, Sanchís Saez, Javier, Garcia-Nieto, Sergio, Simarro Fernández, Raúl. "Recursive Rewarding Modified Adaptive Cell Decomposition (RR-MACD): A Dynamic Path Planning Algorithm for UAVs", 'MDPI AG', 2019</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="45">
	    <p>
	        < 1% match (从 2020年07月28日 的网络)
	    </p>
        
        
 
	    <p><a href="https://mafiadoc.com/arxiv171203747v3-cscv-11-jun-2018_5c19f031097c47073f8b4586.html" target="_blank" style="color:#B64B01">https://mafiadoc.com/arxiv171203747v3-cscv-11-jun-2018_5c19f031097c47073f8b4586.html</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="46">
	    <p>
	        < 1% match (从 2023年02月04日 的网络)
	    </p>
        
        
 
	    <p><a href="https://s3-eu-west-1.amazonaws.com/pstorage-techrxiv-6044451694/28948746/jbhi20210722.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA3OGA3B5WJZPQBW6Z%2F20230204%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Date=20230204T191748Z&X-Amz-Expires=10&X-Amz-Signature=838d9ffe95c4ff0f223be12dfa9879b1b6550cd8845844ea3d1525b853915724&X-Amz-SignedHeaders=host" target="_blank" style="color:#630000">https://s3-eu-west-1.amazonaws.com/pstorage-techrxiv-6044451694/28948746/jbhi20210722.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA3OGA3B5WJZPQBW6Z%2F20230204%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Date=20230204T191748Z&X-Amz-Expires=10&X-Amz-Signature=838d9ffe95c4ff0f223be12dfa9879b1b6550cd8845844ea3d1525b853915724&X-Amz-SignedHeaders=host</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="47">
	    <p>
	        < 1% match (Chenji Zhao, Shun Xiang, Yuanquan Wang, Zhaoxi Cai, Jun Shen, Shoujun Zhou, Di Zhao, Weihua Su, Shijie Guo, Shuo Li. "Context-aware Network Fusing Transformer and V-Net for Semi-supervised Segmentation of 3D Left Atrium", Expert Systems with Applications, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(778217691,37,'0')" target="_blank" style="color:#0270B6">Chenji Zhao, Shun Xiang, Yuanquan Wang, Zhaoxi Cai, Jun Shen, Shoujun Zhou, Di Zhao, Weihua Su, Shijie Guo, Shuo Li. "Context-aware Network Fusing Transformer and V-Net for Semi-supervised Segmentation of 3D Left Atrium", Expert Systems with Applications, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="48">
	    <p>
	        < 1% match (Dan-Sebastian Bacea, Florin Oniga. "Single stage architecture for improved accuracy real-time object detection on mobile devices", Image and Vision Computing, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(782670640,37,'0')" target="_blank" style="color:#330099">Dan-Sebastian Bacea, Florin Oniga. "Single stage architecture for improved accuracy real-time object detection on mobile devices", Image and Vision Computing, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="49">
	    <p>
	        < 1% match (Haoxuan Xu, Songning Lai, Xianyang Li, Yang Yang. "Cross-domain car detection model with integrated convolutional block attention mechanism", Image and Vision Computing, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(808688403,37,'0')" target="_blank" style="color:#227967">Haoxuan Xu, Songning Lai, Xianyang Li, Yang Yang. "Cross-domain car detection model with integrated convolutional block attention mechanism", Image and Vision Computing, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="50">
	    <p>
	        < 1% match (Jiajun Zhu, Man Cheng, Qifan Wang, Hongbo Yuan, Zhenjiang Cai. "Grape Leaf Black Rot Detection Based on Super-Resolution Image Enhancement and Deep Learning", Frontiers in Plant Science, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(721132201,37,'0')" target="_blank" style="color:#CB0099">Jiajun Zhu, Man Cheng, Qifan Wang, Hongbo Yuan, Zhenjiang Cai. "Grape Leaf Black Rot Detection Based on Super-Resolution Image Enhancement and Deep Learning", Frontiers in Plant Science, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="51">
	    <p>
	        < 1% match (Jianwu Long, Chengxin Yang, Yan Ren, Ziqin Zeng. "Semi-supervised medical image segmentation via feature similarity and reliable-region enhancement", Computers in Biology and Medicine, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(808705146,37,'0')" target="_blank" style="color:#006331">Jianwu Long, Chengxin Yang, Yan Ren, Ziqin Zeng. "Semi-supervised medical image segmentation via feature similarity and reliable-region enhancement", Computers in Biology and Medicine, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="52">
	    <p>
	        < 1% match (Jianxing Xiao, Gang Liu, Kejian Wang, Yongsheng Si. "Cow identification in free-stall barns based on an improved Mask R-CNN and an SVM", Computers and Electronics in Agriculture, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(781845938,37,'0')" target="_blank" style="color:#795AB9">Jianxing Xiao, Gang Liu, Kejian Wang, Yongsheng Si. "Cow identification in free-stall barns based on an improved Mask R-CNN and an SVM", Computers and Electronics in Agriculture, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="53">
	    <p>
	        < 1% match (从 2023年01月21日 的网络)
	    </p>
        
        
 
	    <p><a href="https://assets.researchsquare.com/files/rs-2479806/v1/4f7e677e5459fea6c949a382.pdf?c=1674036829" target="_blank" style="color:#935F32">https://assets.researchsquare.com/files/rs-2479806/v1/4f7e677e5459fea6c949a382.pdf?c=1674036829</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="54">
	    <p>
	        < 1% match (从 2020年09月24日 的网络)
	    </p>
        
        
 
	    <p><a href="https://download.atlantis-press.com/journals/ijcis/125944628/view" target="_blank" style="color:#ce0031">https://download.atlantis-press.com/journals/ijcis/125944628/view</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="55">
	    <p>
	        < 1% match (从 2022年09月22日 的网络)
	    </p>
        
        
 
	    <p><a href="https://downloads.hindawi.com/journals/am/2022/3712289.pdf" target="_blank" style="color:#866712">https://downloads.hindawi.com/journals/am/2022/3712289.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="56">
	    <p>
	        < 1% match (从 2023年05月23日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.arxiv-vanity.com/papers/2101.10837/" target="_blank" style="color:#63009c">https://www.arxiv-vanity.com/papers/2101.10837/</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="57">
	    <p>
	        < 1% match (从 2023年08月01日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.frontiersin.org/articles/10.3389/fnins.2023.1212049/full" target="_blank" style="color:#A85503">https://www.frontiersin.org/articles/10.3389/fnins.2023.1212049/full</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="58">
	    <p>
	        < 1% match (从 2022年08月04日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.researchgate.net/publication/362276375_Linguistic_Steganalysis_via_Fusing_Multi-granularity_Attentional_Text_Features" target="_blank" style="color:#cc0066">https://www.researchgate.net/publication/362276375_Linguistic_Steganalysis_via_Fusing_Multi-granularity_Attentional_Text_Features</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="59">
	    <p>
	        < 1% match (Hai Wang, Shilin Zhu, Long Chen, Yicheng Li, Tong Luo. "CompleteInst: An Efficient Instance Segmentation Network for Missed Detection Scene of Autonomous Driving", Sensors, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(808955099,37,'0')" target="_blank" style="color:#21785B">Hai Wang, Shilin Zhu, Long Chen, Yicheng Li, Tong Luo. "CompleteInst: An Efficient Instance Segmentation Network for Missed Detection Scene of Autonomous Driving", Sensors, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="60">
	    <p>
	        < 1% match (Haruki Fujii, Hayato Tanaka, Momoko Ikeuchi, Kazuhiro Hotta. "X-net with Different Loss Functions for Cell Image Segmentation", 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(739214041,37,'0')" target="_blank" style="color:#336699">Haruki Fujii, Hayato Tanaka, Momoko Ikeuchi, Kazuhiro Hotta. "X-net with Different Loss Functions for Cell Image Segmentation", 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="61">
	    <p>
	        < 1% match ("Artificial Neural Networks and Machine Learning – ICANN 2021", Springer Science and Business Media LLC, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(739624806,37,'0')" target="_blank" style="color:#D10A0A">"Artificial Neural Networks and Machine Learning – ICANN 2021", Springer Science and Business Media LLC, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="62">
	    <p>
	        < 1% match ("Medical Image Computing and Computer Assisted Intervention – MICCAI 2021", Springer Science and Business Media LLC, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(740426806,37,'0')" target="_blank" style="color:#287B28">"Medical Image Computing and Computer Assisted Intervention – MICCAI 2021", Springer Science and Business Media LLC, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="63">
	    <p>
	        < 1% match (Arnob Chatterjee, Soumik Chatterjee, Barbara Smith, James E. Cresswell, Parthiba Basu. "Predicted thresholds for natural vegetation cover to safeguard pollinator services in agricultural landscapes", Agriculture, Ecosystems & Environment, 2020)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(644341988,37,'0')" target="_blank" style="color:blue">Arnob Chatterjee, Soumik Chatterjee, Barbara Smith, James E. Cresswell, Parthiba Basu. "Predicted thresholds for natural vegetation cover to safeguard pollinator services in agricultural landscapes", Agriculture, Ecosystems & Environment, 2020</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="64">
	    <p>
	        < 1% match (Chenchen Shi, Changlun Zhang, Lulu Deng, Qiang He, Hengyou Wang, Lianzhi Huo. "A boundary optimization model of instance segmentation combined with wavelet transform on Buildings", Journal of Intelligent & Fuzzy Systems, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(801061871,37,'0')" target="_blank" style="color:brown">Chenchen Shi, Changlun Zhang, Lulu Deng, Qiang He, Hengyou Wang, Lianzhi Huo. "A boundary optimization model of instance segmentation combined with wavelet transform on Buildings", Journal of Intelligent & Fuzzy Systems, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="65" aria-posinset="65">
	    <p>
	        < 1% match (Rongsheng Dong, Lulu Bai, Fengying Li. "SiameseDenseU-Net-based Semantic Segmentation of Urban Remote Sensing Images", Mathematical Problems in Engineering, 2020)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(649270073,37,'0')" target="_blank" style="color:#B64B01">Rongsheng Dong, Lulu Bai, Fengying Li. "SiameseDenseU-Net-based Semantic Segmentation of Urban Remote Sensing Images", Mathematical Problems in Engineering, 2020</a>

    
    </p>
    </div></div></div></div><div id="body" tabIndex="0" role="main"><p>Research on Panoptic Segmentation Algorithm Based on Transformer Abstract <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.dsp.2021.103283', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">Panoptic segmentation is a</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> kind <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.dsp.2021.103283', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> image <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.dsp.2021.103283', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">segmentation</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> task that integrates image <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.dsp.2021.103283', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">semantic segmentation and instance segmentation</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, which not only <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.dsp.2021.103283', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">focuses on</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the stuff of the background such as sky, road, etc., but also focuses on the things of different individuals such as people, car, etc. In order to learn more comprehensive image features, the FPN-style mask head in Mask R-CNN is added on top of the End-to-end Detection Transformer. more comprehensive image features, a new FPN-style mask head in Mask R-CNN is added on top of End-to-end Detection Transformer, which converts the output features of Transformer Decoder into instance segmentation map. Finally, the PQ in COCO 2017 Validation is 42.8 for PQ of stuff and 35.8 for PQ of things, which is in the high level compared with the same period. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">I. Introduction With the development of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> computer hardware <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Internet <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">technology</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, not only are computers able to handle more complex tasks, but also major manufacturers and companies are able to collect more types and amounts of data, sometimes text, sometimes speech, and sometimes images. When more of this data is available, the need for more complex computer tasks gradually arises, and this is the case with image recognition. Early image recognition was to read the image as a matrix, where each pixel point was treated as a feature, so the number of pixel points was the number of features. Limited by the hardware of the device, early image recognition can only accomplish the task of classifying image features on pictures with particularly small pixels, and most algorithms are based on traditional statistics and probability theory reasoning, without substantial breakthroughs. With the emergence of CNNs such as LetNet-5 [1], AlexNet [2], VGG [3], and ResNet [4] at the beginning of the 21st century, the field of image recognition is no longer limited to categorizing images, and more complex <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: http://export.arxiv.org/pdf/2303.07011"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=2066771444&n=3801&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">tasks such as object detection and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> image <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: http://export.arxiv.org/pdf/2303.07011"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=2066771444&n=3801&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">segmentation have</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> been proposed. Earlier <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: http://export.arxiv.org/pdf/2303.07011"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=2066771444&n=3801&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> image segmentation task is <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 65 in source list: Rongsheng Dong, Lulu Bai, Fengying Li. "SiameseDenseU-Net-based Semantic Segmentation of Urban Remote Sensing Images", Mathematical Problems in Engineering, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1155/2020/1515630', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">to classify images at pixel level</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, J. Long, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 30 in source list: Gang Zhang, Chenhong Zheng, Jianfeng He, Sanli Yi. "PCT: Pyramid convolutional transformer for parotid gland tumor segmentation in ultrasound images", Biomedical Signal Processing and Control, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.bspc.2022.104498', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">et al</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. [5] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 30 in source list: Gang Zhang, Chenhong Zheng, Jianfeng He, Sanli Yi. "PCT: Pyramid convolutional transformer for parotid gland tumor segmentation in ultrasound images", Biomedical Signal Processing and Control, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.bspc.2022.104498', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">proposed Full Convolutional Neural Network FCN</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> in 2015, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 30 in source list: Gang Zhang, Chenhong Zheng, Jianfeng He, Sanli Yi. "PCT: Pyramid convolutional transformer for parotid gland tumor segmentation in ultrasound images", Biomedical Signal Processing and Control, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.bspc.2022.104498', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">which</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> is <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 30 in source list: Gang Zhang, Chenhong Zheng, Jianfeng He, Sanli Yi. "PCT: Pyramid convolutional transformer for parotid gland tumor segmentation in ultrasound images", Biomedical Signal Processing and Control, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.bspc.2022.104498', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> first <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 56 in source list: https://www.arxiv-vanity.com/papers/2101.10837/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=26107734&n=3802&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">deep learning based semantic segmentation</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> model. The model <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 56 in source list: https://www.arxiv-vanity.com/papers/2101.10837/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=26107734&n=3802&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">can be</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> divided <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 56 in source list: https://www.arxiv-vanity.com/papers/2101.10837/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=26107734&n=3802&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">into</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 47 in source list: Chenji Zhao, Shun Xiang, Yuanquan Wang, Zhaoxi Cai, Jun Shen, Shoujun Zhou, Di Zhao, Weihua Su, Shijie Guo, Shuo Li. "Context-aware Network Fusing Transformer and V-Net for Semi-supervised Segmentation of 3D Left Atrium", Expert Systems with Applications, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.eswa.2022.119105', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">the structure of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> encoder-decoder, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 47 in source list: Chenji Zhao, Shun Xiang, Yuanquan Wang, Zhaoxi Cai, Jun Shen, Shoujun Zhou, Di Zhao, Weihua Su, Shijie Guo, Shuo Li. "Context-aware Network Fusing Transformer and V-Net for Semi-supervised Segmentation of 3D Left Atrium", Expert Systems with Applications, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.eswa.2022.119105', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> encoder, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 47 in source list: Chenji Zhao, Shun Xiang, Yuanquan Wang, Zhaoxi Cai, Jun Shen, Shoujun Zhou, Di Zhao, Weihua Su, Shijie Guo, Shuo Li. "Context-aware Network Fusing Transformer and V-Net for Semi-supervised Segmentation of 3D Left Atrium", Expert Systems with Applications, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.eswa.2022.119105', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">i.e., the structure</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> of CNN is used <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 61 in source list: "Artificial Neural Networks and Machine Learning – ICANN 2021", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-86365-4', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">to extract the feature map; the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> decoder consists of multiple up-sampling layers, which are up-sampled by <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-16446-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">the feature map to the original</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> image <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-16446-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">size</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-16446-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> segmentation map, so <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-16446-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> subsequently proposed <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 62 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2021", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-87193-2', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">U-Net [6], V-Net</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> [7] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 62 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2021", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-87193-2', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> other <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 55 in source list: https://downloads.hindawi.com/journals/am/2022/3712289.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=680441330&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">deep learning models for semantic segmentation</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> are designed <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 55 in source list: https://downloads.hindawi.com/journals/am/2022/3712289.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=680441330&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">based</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> on <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 55 in source list: https://downloads.hindawi.com/journals/am/2022/3712289.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=680441330&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> encoder-decoder structure. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: Xiaole Fan, Xiufang Feng. "SELDNet: Sequenced encoder and lightweight decoder network for COVID-19 infection region segmentation", Displays, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.displa.2023.102395', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">With the development of image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> semantic <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: Xiaole Fan, Xiufang Feng. "SELDNet: Sequenced encoder and lightweight decoder network for COVID-19 infection region segmentation", Displays, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.displa.2023.102395', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">segmentation</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, scholars believe that the sky, roads and pedestrians, vehicles can’t be confused, B. Hariharan, et al. [8] that there should exist a method to be able to segment the target object Things for pedestrians, vehicles, that is, instance segmentation in the middle of the development of image <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.dsp.2021.103283', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">segmentation. Instance segmentation is different from semantic segmentation</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, which <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.dsp.2021.103283', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">is to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> form a segmentation map of target objects on the detected target, which can be understood as a two-stage task of semantic segmentation of objects of object frames after generating object prediction frames from object detection, so scholars add FCN decoder similar structure on the object detection model in order to generate target segmentation map. Until 2017 K. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 59 in source list: Hai Wang, Shilin Zhu, Long Chen, Yicheng Li, Tong Luo. "CompleteInst: An Efficient Instance Segmentation Network for Missed Detection Scene of Autonomous Driving", Sensors, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3390/s23229102', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">He, et al</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. [9] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 59 in source list: Hai Wang, Shilin Zhu, Long Chen, Yicheng Li, Tong Luo. "CompleteInst: An Efficient Instance Segmentation Network for Missed Detection Scene of Autonomous Driving", Sensors, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3390/s23229102', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">proposed Mask R-CNN</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> more flexible and simple <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 64 in source list: Chenchen Shi, Changlun Zhang, Lulu Deng, Qiang He, Hengyou Wang, Lianzhi Huo. "A boundary optimization model of instance segmentation combined with wavelet transform on Buildings", Journal of Intelligent & Fuzzy Systems, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3233/JIFS-222312', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">two-stage instance segmentation</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> model based on <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 64 in source list: Chenchen Shi, Changlun Zhang, Lulu Deng, Qiang He, Hengyou Wang, Lianzhi Huo. "A boundary optimization model of instance segmentation combined with wavelet transform on Buildings", Journal of Intelligent & Fuzzy Systems, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3233/JIFS-222312', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">R-CNN</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> framework. The model achieves better experimental results on MS COCO [10] dataset, adopts ResNet-FPN structure, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: https://www.hindawi.com/journals/js/2021/6659831/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=757584660&n=3796&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">i.e., multi-layer feature</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> map <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: https://www.hindawi.com/journals/js/2021/6659831/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=757584660&n=3796&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">fusion</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> in FPN, whose <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: https://www.hindawi.com/journals/js/2021/6659831/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=757584660&n=3796&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">multi-scale feature</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> maps are conducive to multi-scale objects as well as small objects detection, and finally obtains the instance segmentation maps by bilinear interpolation up sampling, which has good segmentation quality for object targets. With the development of single-stage algorithms for object detection, single-stage instance segmentation is also emerging, as represented by YOLCAT [11], which adds the mask generation method to the single-stage object detection model. On the basis <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.dsp.2021.103283', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">of instance segmentation and semantic segmentation</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: Xinye Li, Ding Chen. "A survey on deep learning-based panoptic segmentation", Digital Signal Processing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.dsp.2021.103283', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">panoptic segmentation</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> tasks of Unifing Things and Stuff were derived, which added pixel-level segmentation of the backscene on the basis of instance segmentation. However, the distinction between foreground and rear view will lead to poor definition of evaluation metric, so A. Kirillov, et al. [12] proposed <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 40 in source list: Xiaolong Liu, Yuqing Hou, Anbang Yao, Yurong Chen, Keqiang Li. "CASNet: Common Attribute Support Network for image instance and panoptic segmentation", 2020 25th International Conference on Pattern Recognition (ICPR), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICPR48806.2021.9412635', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">the evaluation metric for panoptic segmentation: Panoptic Quality (PQ</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">), which is a unification of Things and Stuff's evaluation metric for panoptic segmentation, and it can evaluate the model's performance in panoptic segmentation in a more effective and comprehensive way. The development <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 38 in source list: "Image and Video Technology", Springer Science and Business Media LLC, 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-34879-3', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">in the field of computer vision with the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> victory <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 38 in source list: "Image and Video Technology", Springer Science and Business Media LLC, 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-34879-3', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> convolutional neural network in the ImageNet competition in 2012 has accelerated the development of various fields <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: "Image and Graphics", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-71607-7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">based on convolutional neural network</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, at <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: "Image and Graphics", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-71607-7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> same time <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 54 in source list: https://download.atlantis-press.com/journals/ijcis/125944628/view"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=1848045053&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">the field of computer vision</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> has brought about <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 54 in source list: https://download.atlantis-press.com/journals/ijcis/125944628/view"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=1848045053&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">deep learning</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, which has led <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 54 in source list: https://download.atlantis-press.com/journals/ijcis/125944628/view"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=1848045053&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 58 in source list: https://www.researchgate.net/publication/362276375_Linguistic_Steganalysis_via_Fusing_Multi-granularity_Attentional_Text_Features"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=4150515112&n=3798&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">the rapid development of natural language processing. In</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 2017 Vaswani A. et al [13] published <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: https://www.hindawi.com/journals/cmmm/2022/7852958/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=1902591443&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">in the field of Natural Language Processing</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Transformer model, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: https://www.hindawi.com/journals/cmmm/2022/7852958/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=1902591443&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">which</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> introduces an encoder-decoder structure with an attention mechanism for sequence-to-sequence modeling of machine translation tasks. Compared with previous sequence-to-sequence models, Transformer accomplishes sentence prediction with varying lengths, reflecting the model's strong generalization ability, which is derived to the greatest extent from the ability of the attention mechanism to perceive global features. In view of Transformer's excellent global perception ability, A. Dosovitskiy, et al. [14] first proposed to use the Transformer encoder in an image classification task by flattening each piece of a chunked image and treating it as a single Token of the input vector, and finally adding a multilayer perceptron to realize the classification task, and the subsequent experiments confirmed that such a The subsequent experiments confirmed that this kind of architecture abandons convolution, which makes the model accuracy much higher than the traditional convolutional neural network, so Transformer formally enters the vision of scholars <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 42 in source list: Zhihong Xu, Mingyang Fan, Yapeng Zhang. "Visual detection of eggs based on deep learning for egg picking robot", Journal of Physics: Conference Series, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1088/1742-6596/1976/1/012013', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">in the field of computer vision</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. 2020 N. Carion, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 42 in source list: Zhihong Xu, Mingyang Fan, Yapeng Zhang. "Visual detection of eggs based on deep learning for egg picking robot", Journal of Physics: Conference Series, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1088/1742-6596/1976/1/012013', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">et al</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. [15] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 42 in source list: Zhihong Xu, Mingyang Fan, Yapeng Zhang. "Visual detection of eggs based on deep learning for egg picking robot", Journal of Physics: Conference Series, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1088/1742-6596/1976/1/012013', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">proposed</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the end-to-end <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 25 in source list: "Pattern Recognition and Computer Vision", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-88013-2', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">object detection model</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> DETR <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 25 in source list: "Pattern Recognition and Computer Vision", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-88013-2', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">based on Transformer</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, which eliminates the tedious Archer Box pre-setup in the architectures such as R-CNN, YOLCAT, and so on. The cumbersome Archer Box pre-setting and Non- Maximum suppression post-processing are eliminated, and the end-to-end object detection depth model is truly realized by objects queries. Although its performance in object detection is not as good as other architectures in the same period, this DETR architecture without cumbersome pre/post-processing became the pioneering work of this model, which has been widely used by subsequent scholars, such as the segmentation architectures SETR proposed in recent years [16], MaskFormer [17], are end-to-end architectures based on Transformer, which is based on convolutional neural network local receptive field and Transformer global features, which can understand the image features more comprehensively and thus accomplish more complex visual tasks. In this paper, it will introduce a Mask R-CNN style Mask head based on the framework of DETR to process the output of Transformer Decoder for panoptic segmentation task, and perform model replication and algorithm profiling at COCO 2017 Panoptic Validation. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 23 in source list: https://d-nb.info/119244177X/34"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=13.397454258094&svr=6&lang=zh_hans&sid=55302010&n=3783&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">Due to the large number of parameters in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the Transformer architecture itself that makes it difficult to train, the pre-trained model that has been completed by ResNet in the ImageNet competition is taken as a base on Backbone to reduce the training time required for the convergence of model parameters. II. Methods This chapter will introduce the underlying DETR architecture and how to introduce the Mask Head <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 52 in source list: Jianxing Xiao, Gang Liu, Kejian Wang, Yongsheng Si. "Cow identification in free-stall barns based on an improved Mask R-CNN and an SVM", Computers and Electronics in Agriculture, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.compag.2022.106738', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">of the Mask R-CNN</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> into <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 52 in source list: Jianxing Xiao, Gang Liu, Kejian Wang, Yongsheng Si. "Cow identification in free-stall barns based on an improved Mask R-CNN and an SVM", Computers and Electronics in Agriculture, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.compag.2022.106738', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">the structure</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, and dissect <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 52 in source list: Jianxing Xiao, Gang Liu, Kejian Wang, Yongsheng Si. "Cow identification in free-stall barns based on an improved Mask R-CNN and an SVM", Computers and Electronics in Agriculture, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.compag.2022.106738', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> loss function and evaluation metrics required for training. A. DETR architecture DETR <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">is mainly composed of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> three <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">parts</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, Backbone, Transformer, and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> output FFN of shared parameters, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">and the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> original <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">structure is shown in Fig. 1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Figure <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. DETR original <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: "Conference Proceeding", 2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ISPACS51563.2021.9650930', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">structure</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. (1) Backbone This part is to use CNN <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: "Neural Information Processing", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70096-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">to extract features from the original image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, remove the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: "Neural Information Processing", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70096-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">global average pooling, fully connected layer</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: "Neural Information Processing", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70096-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> CNN final output classifier, and perform deep convolution operation on the input three-channel color image, i.e., the three-channel image features are mapped to the features of more number of channels to get the multi-channel feature map. Assuming that <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: Sirohi, Kshitij, Mohan, Rohit, Büscher, Daniel, Burgard, Wolfram, Valada, Abhinav. "EfficientLPS: Efficient LiDAR Panoptic Segmentation", 2021"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1074400790&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">the width</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> H <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: Sirohi, Kshitij, Mohan, Rohit, Büscher, Daniel, Burgard, Wolfram, Valada, Abhinav. "EfficientLPS: Efficient LiDAR Panoptic Segmentation", 2021"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1074400790&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">and height</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> W pixels <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: Sirohi, Kshitij, Mohan, Rohit, Büscher, Daniel, Burgard, Wolfram, Valada, Abhinav. "EfficientLPS: Efficient LiDAR Panoptic Segmentation", 2021"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1074400790&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">of the input image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> are 224, a more representative ResNet-50 forward is shown in Fig. 2. Figure 2. ResNet-50 forward calculation. Backbone is the structure that removes the last yellow square, leaving only 2D convolutional, pooling layers. When initializing the kernel weight parameters of each convolutional layer, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 36 in source list: Alturbeh, Hamid, Stow, Julian, Tucker, Gareth, Lawton, Alan. "Modelling and simulation of the train brake system in low adhesion conditions", 'SAGE Publications', 2020"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1472391994&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">in order to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> reduce <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 36 in source list: Alturbeh, Hamid, Stow, Julian, Tucker, Gareth, Lawton, Alan. "Modelling and simulation of the train brake system in low adhesion conditions", 'SAGE Publications', 2020"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1472391994&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> convergence <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 36 in source list: Alturbeh, Hamid, Stow, Julian, Tucker, Gareth, Lawton, Alan. "Modelling and simulation of the train brake system in low adhesion conditions", 'SAGE Publications', 2020"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1472391994&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">time required for model</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> training, the parameters <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 36 in source list: Alturbeh, Hamid, Stow, Julian, Tucker, Gareth, Lawton, Alan. "Modelling and simulation of the train brake system in low adhesion conditions", 'SAGE Publications', 2020"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1472391994&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">are</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> initialized <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 36 in source list: Alturbeh, Hamid, Stow, Julian, Tucker, Gareth, Lawton, Alan. "Modelling and simulation of the train brake system in low adhesion conditions", 'SAGE Publications', 2020"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1472391994&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the form of a normal distribution. In the selection of Backbone, considering the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 45 in source list: https://mafiadoc.com/arxiv171203747v3-cscv-11-jun-2018_5c19f031097c47073f8b4586.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=19433031&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">number of parameters of the overall model</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, it should choose <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 45 in source list: https://mafiadoc.com/arxiv171203747v3-cscv-11-jun-2018_5c19f031097c47073f8b4586.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=19433031&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> structure with as <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 45 in source list: https://mafiadoc.com/arxiv171203747v3-cscv-11-jun-2018_5c19f031097c47073f8b4586.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=19433031&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">few</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> convolutional layers as possible, with the best performance as possible, and with the subsequent Transformer structure, so chosing the ResNet series of networks as Backbone, which takes more original features into consideration. In addition to the convolutional structure, each layer also incorporates the residual connection, which accumulates the outputs of the previous module with the same size to the next module, and then adds the outputs of the next module with the same size to the next module. output of one module is accumulated to the next output feature map, so that the model can learn some of the features before the convolution operation. (2) Positional Encoding CNN has excellent image feature extraction, but the extracted multi-channel feature maps do not have good relative position information, i.e., the features do not have informative features between them, while the input to the Transformer model is word vectors with contextual relationships, so it is necessary to add position information i.e., positional coding to the multi-channel features. Based on the idea of bottleneck layer in residual network, the position information is accumulated to the feature vector and the feature vector has position information. There are two methods of position encoding in Transformer: the first is based on the idea of word index vector Embedding, which gives a learnable weight matrix to the input vectors, and the position encoding is obtained by learning while training the model parameters; the second method is based on the position encoding vectors generated by empirical formulas. Mask is required before generating the positional coding, as each image in the dataset has a different size, the training process is a batch training approach, the Transformer's features can be input with different lengths of the features, but it is necessary to iterate all the images in the current batch filled with blank pixel points with a value of 0 to the maximum width and height in the current batch of images in order to form a uniform size. Because of the filled blank pixels, the training process needs to add a set of feature Masks in the input process in order to prevent the blank pixels from interfering with the learning of the Transformer parameters, and to perform Masking operation for the subsequent Multi-head Attention (MHA). When the batch size is 2, the feature Masks are generated as in Fig. 3. Figure 3. Generate batch feature mask. Where the position where the Mask element is False indicates that the position feature does not need to be masked, on the contrary when the element is True an infinitesimal number is accrued to the corresponding position of the feature vector in the MHA's Masking so that the feature map in the subsequent linear mapping of the multiple attention the corresponding value is also infinitesimal so that the attention weight is close to 0 at the corresponding position. To generate the Positional Encoding, a sine-cosine function will be taken. The binarized vector Not- Mask is generated according to the Mask with its opposite, i.e., the elements of the original position are changed from False to True, and True to False, and the Not-Mask is copied d times in order to initialize the Positional Encoding Vector 𝑃𝐸 ∈ 𝑃𝑏×32×32 at the same size as the feature map. Subsequent 𝐻 𝑊 Cumulative sum for 𝑏 𝐻 2 × × 𝑊 cumulative and horizontal coordinate directions, and the remaining 32 32 cumulative and vertical coordinates, will result in two sets of vectors of relative positions as in Fig. 4. Figure 4. Two parts relative positions vector. The embedding dimension d is equally divided into two parts for cumulative sums in different directions to achieve simultaneous consideration of the relationship between <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: Xiaole Fan, Xiufang Feng. "SELDNet: Sequenced encoder and lightweight decoder network for COVID-19 infection region segmentation", Displays, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.displa.2023.102395', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">the horizontal and vertical coordinates</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> in <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: Xiaole Fan, Xiufang Feng. "SELDNet: Sequenced encoder and lightweight decoder network for COVID-19 infection region segmentation", Displays, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.displa.2023.102395', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">the image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> plane. Subsequently, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: Xiaole Fan, Xiufang Feng. "SELDNet: Sequenced encoder and lightweight decoder network for COVID-19 infection region segmentation", Displays, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.displa.2023.102395', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> elements of PE are nonlinearly transformed using the sine-cosine function to generate a positional code with relative positional information. For the position pos, the value 𝑃𝐸(𝑜𝑜𝑠, 𝑖) of the dimension index i is as in equation (1). 𝑜𝑜𝑠 sin ( 2𝑘) , 𝑖 = 2𝑘, 𝑃𝐸(𝑜𝑜𝑠, 𝑖) = 10000 𝑏 𝑜𝑜𝑠 (1) cos ( { 2𝑘 ) , 𝑖 = 2𝑘 + 1 10000 𝑏 (3) Transformer The Transformer structure in DETR is similar to the original structure in that the input vectors are adjusted to be 3D features in <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 25 in source list: "Pattern Recognition and Computer Vision", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-88013-2', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">the shape of 𝑏 × 𝐻 × 𝑊</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> , whereas <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 25 in source list: "Pattern Recognition and Computer Vision", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-88013-2', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">the input</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> vectors of the original structure 32 32 are in the shape of 2D, so 𝐻 × 𝑊 is flattened, i.e., the 3D features are transformed into Flatten Token in 32 32 the shape of 𝑏 × ( 𝐻 × 𝑊 ).Before entering the Transformer, an additional convolution layer of 1 × 1 32 32 kernel size is required to achieve a certain degree of dimensionality reduction of the multichannel features. Transformer needs to go through an additional <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: "Image and Graphics", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-71607-7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">convolutional layer</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: "Image and Graphics", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-71607-7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">1 × 1 kernel size to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> achieve <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: "Image and Graphics", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-71607-7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">a</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> certain degree of dimensionality reduction of the multi-channel features. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 33 in source list: Xiaodong Liu, Haipeng Guo, Huanyu Liu, Junbao Li. "Domain migration representation learning for blind magnetic resonance image super-resolution", Biomedical Signal Processing and Control, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.bspc.2023.105357', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">The structure of Transformer Encoder</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 33 in source list: Xiaodong Liu, Haipeng Guo, Huanyu Liu, Junbao Li. "Domain migration representation learning for blind magnetic resonance image super-resolution", Biomedical Signal Processing and Control, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.bspc.2023.105357', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">Decoder</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> in DETR <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 33 in source list: Xiaodong Liu, Haipeng Guo, Huanyu Liu, Junbao Li. "Domain migration representation learning for blind magnetic resonance image super-resolution", Biomedical Signal Processing and Control, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.bspc.2023.105357', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">is shown in Fig</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. 5. Figure 5. Detailed structure of Transformer in DETR. The Encoder and Decoder of Transformer are composed of 𝐾enc Encoder blocks and 𝐾dec decoder blocks respectively, and each encoder block and decoder block have similar structure with MHA, residual add, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-16446-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">Layer Normalization (LN), Feed Forward Network (FFN), the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> decoder block has one more MHA than the encoder block to accept <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: https://web.archive.org/web/20220810064324if_/https:/arxiv.org/pdf/2208.00662v2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1115397144&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">the output of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> encoder, and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: https://web.archive.org/web/20220810064324if_/https:/arxiv.org/pdf/2208.00662v2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1115397144&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">Multi-head Self Attention (MHSA</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) is essentially <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: https://web.archive.org/web/20220810064324if_/https:/arxiv.org/pdf/2208.00662v2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1115397144&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> same as the MHA. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: Pan, Shaoyan, Liu, Yiqiao et al. "Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in  Multiple Anatomical Locations", 2023"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1488608644&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">Multi-head Self Attention (MHSA) is</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> essentially <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: Pan, Shaoyan, Liu, Yiqiao et al. "Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in  Multiple Anatomical Locations", 2023"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1488608644&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> same as MHA, except that MHSA focuses more on "self-attention", and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://www.hindawi.com/journals/ijis/2023/6266209/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=2187343417&n=3801&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">the detailed structure of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> MHA <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://www.hindawi.com/journals/ijis/2023/6266209/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=2187343417&n=3801&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">is shown in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Fig. 6. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://www.hindawi.com/journals/ijis/2023/6266209/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=2187343417&n=3801&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">Figure</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 6. MHA and Scaled Dot-product Attention. MHA is essentially a soft query, which is a linear mapping of input vectors through Fully Connected (FC) to get three different matrices 𝑃, 𝐾, and 𝑇. In the model, 𝐾 and 𝑇 are from the same input vector, and 𝑃 is mapped from the same input vector as MHSA. In MHA, the input vectors with larger shapes are split into 𝐾 heads for concatenation computation, and then finally spliced together after a linear mapping to get the same shape as the input vectors. The input vector with the same shape as the input vector is obtained by a linear mapping. In the attention of a single head, the Scaled Dot-product Attention operation is adopted, which is calculated according to 𝑃 ⋅ 𝐾𝑇 to obtain a square matrix, and after scaled reduction, Masking, Softmax, it becomes a weight matrix with element values within [0,1], i.e., the attention weights. The new vector obtained after multiplying the attention weights with V is the feature vector processed according to the attention weights generated by 𝑃. Define the input vector as 𝑋. The whole MHA formula is shown in Eqs. (2-3). MHA(𝑋𝑜, 𝑋𝑘𝑣 , 𝑇, 𝐾) = 𝐾 ⋅ [Attention(𝑋𝑜, 𝑋𝑘𝑣, 𝑇1); ⋯ ; Attention(𝑋𝑜, 𝑋𝑘𝑣, 𝑇𝐿)] (2) (𝑇𝑚2(𝑋𝑘𝑣 + 𝑃𝐸))𝑇 (𝑇𝑚1(𝑋𝑜 + 𝑃𝐸)) Attention(Xq, 𝑋𝑘𝑣, 𝑇𝑚) = 𝑇𝑚1𝑋𝑘𝑣 ⋅ Softmax ( √𝑏 ) (3) Where, 𝑇𝑚 = {𝑇𝑚1, 𝑇𝑚2, 𝑇𝑚3}, 𝑚 = 1,2, . . . , 𝐾 denotes the three FC layer weight matrices before the Scaled Dot-product Attention in the 𝑚-th header. 𝐾 denotes the weight of the FC layer after the connection. This leads to MHSA(𝑋, 𝑇, 𝐾) = MHA(𝑋, 𝑋, 𝑇, 𝐾). After MHA computation, the features are subjected to residual accumulation and layer normalization to enhance the original features to prevent too much loss in the computation process. In a single encoder or decoder block, an FFN consisting of FC-ReLU-Dropout-FC is included to propagate the MHA computed features forward after two successive linear mappings, in which the two FC layers have the weight dimension of 𝑏𝑏𝑏, i.e., the feature vector with the input dimension of d is mapped into the 𝑏𝑏𝑏 dimension space through the first FC layer. ff dimension space, and mapped back to the d dimension space with the same shape as the input vector through the second FC layer. In the original Transformer model used for Machine Translation, the input to the decoder starts from a 1 × 1 vector of start symbol token, predicts the next word and adds it to the current vector to get a 2 × 1 vector of tokens, which is then fed into the decoder, and so on until the next word predicted is the end symbol token position. But for DETR, only class and bounding box need to be predicted on object detection, so it is sufficient to pass the FFN with shared parameters after decoder. In the input part of the decoder, the original author is defining the object queries, which are essentially the weights of the Embedding layer, to complete the cumbersome anchor box preprocessing in the previous object detection in a learnable way, thus reducing the dependence on empirical people and realizing the end-to-end simplicity, the only drawback is the increase of the The only drawback is that it increases the convergence time of the model parameters during the training process. B. Mask head DETR is an architecture for object detection, using this framework as a basis to accomplish panoptic segmentation, as Mask R-CNN adds Mask head to the shared parameter FFN part to generate the segmentation map, and the mask head <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 4 in source list: https://www.hindawi.com/journals/ijis/2023/8879622/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=475684157&n=3805&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">structure is shown in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Fig. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 4 in source list: https://www.hindawi.com/journals/ijis/2023/8879622/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=475684157&n=3805&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">7. Figure 7</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Mask head <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 4 in source list: https://www.hindawi.com/journals/ijis/2023/8879622/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=475684157&n=3805&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">structure</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Convolutional Mult-head Attention is a method of processing encoder and decoder outputs defined by MHA, which uses its soft query property to convert the features of object queries and encoder outputs into "attention" features of objects. "Attention" features, in general, is to treat stuff and things as the same type of objects, and generate feature maps for 𝐾 = 𝐾𝑜 objects. Masking is similar to MHA's Masking, which also accumulates an infinitesimal number of blank pixels to achieve the purpose of ignoring these blank pixels. When the model parameters converge, these "attention" features can well represent <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 43 in source list: https://dokumen.pub/progress-in-computer-recognition-systems-1st-ed-978-3-030-19737-7978-3-030-19738-4.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=3484527372&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">the position and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> volume <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 43 in source list: https://dokumen.pub/progress-in-computer-recognition-systems-1st-ed-978-3-030-19737-7978-3-030-19738-4.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=3484527372&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">of the object in the image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, so the subsequent FPN-Style CNN is adopted, according to the number of channels of each <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 32 in source list: Tao Zhang, Bo Jin, Wenjing Jia. "An anchor-free object detector based on soften optimized bi-directional FPN", Computer Vision and Image Understanding, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.cviu.2022.103410', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">layer of Backbone's feature map, the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> "attention" <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 32 in source list: Tao Zhang, Bo Jin, Wenjing Jia. "An anchor-free object detector based on soften optimized bi-directional FPN", Computer Vision and Image Understanding, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.cviu.2022.103410', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">feature map is</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> convolved with the "attention" <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 34 in source list: Yangdong Chen, Zhaolong Zhang, Yanfei Wang, Yuejie Zhang, Rui Feng, Tao Zhang, Weiguo Fan. "AE-Net: Fine-grained sketch-based image retrieval via attention-enhanced network", Pattern Recognition, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.patcog.2021.108291', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">feature map, and the "attention" feature map</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> is convolved <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 34 in source list: Yangdong Chen, Zhaolong Zhang, Yanfei Wang, Yuejie Zhang, Rui Feng, Tao Zhang, Weiguo Fan. "AE-Net: Fine-grained sketch-based image retrieval via attention-enhanced network", Pattern Recognition, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.patcog.2021.108291', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">with the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> number <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 34 in source list: Yangdong Chen, Zhaolong Zhang, Yanfei Wang, Yuejie Zhang, Rui Feng, Tao Zhang, Weiguo Fan. "AE-Net: Fine-grained sketch-based image retrieval via attention-enhanced network", Pattern Recognition, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.patcog.2021.108291', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> channels of each layer of Backbone's feature map. According to <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 31 in source list: Mengchao Zhang, Yuan Zhang, Manshan Zhou, Kai Jiang, Hao Shi, Yan Yu, Nini Hao. "Application of Lightweight Convolutional Neural Network for Damage Detection of Conveyor Belt", Applied Sciences, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3390/app11167282', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">the number of channels in each layer of Backbone</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">'s <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 31 in source list: Mengchao Zhang, Yuan Zhang, Manshan Zhou, Kai Jiang, Hao Shi, Yan Yu, Nini Hao. "Application of Lightweight Convolutional Neural Network for Damage Detection of Conveyor Belt", Applied Sciences, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3390/app11167282', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">feature</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> map, the convolution operation is performed on the "attention" <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: https://web.archive.org/web/20210410111526if_/https:/arxiv.org/pdf/2103.14969v1.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1111156388&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">feature map and the corresponding</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> layer <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: https://web.archive.org/web/20210410111526if_/https:/arxiv.org/pdf/2103.14969v1.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1111156388&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">feature map</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> is summed up <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: https://web.archive.org/web/20210410111526if_/https:/arxiv.org/pdf/2103.14969v1.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1111156388&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> learn <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: https://web.archive.org/web/20210410111526if_/https:/arxiv.org/pdf/2103.14969v1.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=1111156388&n=3799&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> details in the original image. After the convolution operation followed by a 2-fold Nearest Interpolation up-sampling method, gradually convert the 𝐻 × 𝑊 instance feature maps to 𝐻 𝑊 32 32 4 × 4 instance feature maps, and finally use the instance <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 29 in source list: https://link.springer.com/article/10.1007/s11042-021-11138-x?code=d5f07e4d-17f2-430e-a4a3-0934df95263a&error=cookies_not_supported"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=2213296274&n=3797&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">feature maps</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> with <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 29 in source list: https://link.springer.com/article/10.1007/s11042-021-11138-x?code=d5f07e4d-17f2-430e-a4a3-0934df95263a&error=cookies_not_supported"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=2213296274&n=3797&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">a convolution layer</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 29 in source list: https://link.springer.com/article/10.1007/s11042-021-11138-x?code=d5f07e4d-17f2-430e-a4a3-0934df95263a&error=cookies_not_supported"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=50.2438305998076&svr=6&lang=zh_hans&sid=2213296274&n=3797&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">3 × 3 kernel size, and the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> masks of each object will be obtained after the convolution operation, according to which the masks can be obtained from the objects in the 𝐻 4 × 𝑊 4 size map of the object's relative position and region information. Finally, the generated object masks are screened by Pixel-wise Argmax to fuse a mask with different instance ids, and finally Bilinear interpolation is taken to restore the instance id masks to the original image size. As mentioned above, Convolutional Multi-head Attention, which is similar to the MHA soft query mechanism, is used in the mask head to accomplish the pixel segmentation of the object, so the decoder outputs can be based on the FPN of the shared parameters at the same time in order to accomplish the object category and the bound box prediction, i.e., the 𝐾𝑜 feature maps are obtained. The obtained 𝐾𝑜 masks, classes and bounding boxes are one-to-one correspondence without any other post-processing, which saves a lot of workload to some extent. C. Loss function Due to the shared parameters FFN and mask head of DETR, three types of losses, segmentation, box, and class, are used for model training as in equation (4). 𝐾 = 𝜆seg𝐾seg + 𝜆box𝐾box + 𝜆cls𝐾cls (4) (a) Hungarian matcher In the inference phase of the model, a collection of N predictions is obtained which are matched with Ground Truth (GT) <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: "Computer Vision – ECCV 2020", Springer Science and Business Media LLC, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-58452-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">in order to compute the loss</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, so a Hungarian matcher is needed to match the predictions with GT. The algorithm produces <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.measurement.2023.114000', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">an optimal</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> bisection <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.measurement.2023.114000', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">match between the predicted and real</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> objects, defining the set of matches as in equation (5). 𝐿 𝜎 =̂ arg min ∑ 𝐾match(𝑦𝑖, 𝑦𝜎̂(𝑖)) (5) 𝜎∈𝔖𝑁 𝑖 𝐾match(𝑦𝑖, 𝑦𝜎̂(𝑖)) = −<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 27 in source list: Diego A. Velazquez, Josep M. Gonfaus, Pau Rodriguez, F. Xavier Roca, Seiichi Ozawa, Jordi Gonzalez. "Logo Detection with No Priors", IEEE Access, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ACCESS.2021.3101297', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">I{𝑏𝑖</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">≠∅}𝑜𝜎̂(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 27 in source list: Diego A. Velazquez, Josep M. Gonfaus, Pau Rodriguez, F. Xavier Roca, Seiichi Ozawa, Jordi Gonzalez. "Logo Detection with No Priors", IEEE Access, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ACCESS.2021.3101297', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">𝑖)(𝑏𝑖) + I{𝑏𝑖</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">≠∅}ℒ𝑏𝑜𝑥(𝑏𝑖, 𝑏𝜎̂(𝑖)) (6) where 𝜎 ̂denotes the optimal pairing of the set of 𝐾 predicted objects 𝜎 ∈ 𝔖𝐿 searched in the set of GTs with the lowest loss; 𝑦 ̂denotes <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 51 in source list: Jianwu Long, Chengxin Yang, Yan Ren, Ziqin Zeng. "Semi-supervised medical image segmentation via feature similarity and reliable-region enhancement", Computers in Biology and Medicine, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.compbiomed.2023.107668', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">the set of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> N <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 51 in source list: Jianwu Long, Chengxin Yang, Yan Ren, Ziqin Zeng. "Semi-supervised medical image segmentation via feature similarity and reliable-region enhancement", Computers in Biology and Medicine, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.compbiomed.2023.107668', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">predicted</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> objects; <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 51 in source list: Jianwu Long, Chengxin Yang, Yan Ren, Ziqin Zeng. "Semi-supervised medical image segmentation via feature similarity and reliable-region enhancement", Computers in Biology and Medicine, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.compbiomed.2023.107668', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">𝑦</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> denotes <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 51 in source list: Jianwu Long, Chengxin Yang, Yan Ren, Ziqin Zeng. "Semi-supervised medical image segmentation via feature similarity and reliable-region enhancement", Computers in Biology and Medicine, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.compbiomed.2023.107668', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">the set of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> GTs populated by 𝜙 (non-object classes) by the number up to 𝐾; and 𝐾match(𝑦𝑖,𝑦𝜎̂(𝑖)) denotes the computation of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 53 in source list: https://assets.researchsquare.com/files/rs-2479806/v1/4f7e677e5459fea6c949a382.pdf?c=1674036829"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1938902272&n=3800&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">between the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> true <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 53 in source list: https://assets.researchsquare.com/files/rs-2479806/v1/4f7e677e5459fea6c949a382.pdf?c=1674036829"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1938902272&n=3800&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">value</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 𝑦𝑖 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 53 in source list: https://assets.researchsquare.com/files/rs-2479806/v1/4f7e677e5459fea6c949a382.pdf?c=1674036829"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1938902272&n=3800&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">and the predicted value</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 𝑦𝜎̂(𝑖) <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 53 in source list: https://assets.researchsquare.com/files/rs-2479806/v1/4f7e677e5459fea6c949a382.pdf?c=1674036829"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1938902272&n=3800&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> its matching Pair-Wise Matching Cost, which is a matching cost that takes into account the similarity between categories, bounding boxes and GTs at the same time. For each element of 𝑦, define 𝑦𝑖 = {𝑏𝑖, 𝑏𝑖}; where 𝑏𝑖 denotes the category label of the object (including 𝜙); and 𝑏𝑖 ∈ [0,1]4 denotes the coordinates of the center of the true frame, its vector of widths and heights with respect to the image size. The Pair-Wise Matching Cost for a predicted category 𝑏𝑖 with probability 𝑜𝜎̂(𝑖)(𝑏𝑖) for 𝜎(𝑖) and a prediction frame 𝑏𝜎̂(𝑖) is shown in Equation (6). The combined loss of each 𝑦𝜎̂(𝑖) and each object of 𝑦 in the category, bounding box is calculated according to𝐾match(𝑦𝑖, 𝑦𝜎̂(𝑖)) and the predicted box 𝑏𝜎̂(𝑖) with the smallest loss is the best matching box for the real box 𝑏𝑖. Then the Hungarian loss of all the above pairings is calculated as in equation (7). 𝐿 ℒHungarian(𝑦, 𝑦)̂ = ∑[− log 𝑜𝜎̂(̂𝑖)(𝑏𝑖) + 𝐼{𝑏𝑖≠∅}𝐾𝑏𝑜𝑥(𝑏𝑖, 𝑏𝜎̂(̂𝑖))] (7) 𝑖=1 (b) <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: "Computer Vision – ECCV 2020", Springer Science and Business Media LLC, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-58452-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">Bounding box loss The second part of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> above Hungarian <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: "Computer Vision – ECCV 2020", Springer Science and Business Media LLC, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-58452-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">matching</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> algorithm <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.measurement.2023.114000', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">is to score the bounding box</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, since <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.measurement.2023.114000', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">DETR</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> is <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.measurement.2023.114000', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">directly</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> predicting <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.measurement.2023.114000', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> bounding <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.measurement.2023.114000', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">box</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> by calculating <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Zhaoyang Qu, Jiye Zang, Yunchang Dong, Nan Qu, Siqi Song, Tao Jiang, Min Li, Feng Liang, Lingcong Li. "An efficient multi-order cascade distillation model for the detection of small targets and occluded objects in transmission line inspection", Measurement, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.measurement.2023.114000', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> relative error 𝐾1 paradigm loss and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 48 in source list: Dan-Sebastian Bacea, Florin Oniga. "Single stage architecture for improved accuracy real-time object detection on mobile devices", Image and Vision Computing, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.imavis.2022.104613', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">intersection over union</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ratio <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 48 in source list: Dan-Sebastian Bacea, Florin Oniga. "Single stage architecture for improved accuracy real-time object detection on mobile devices", Image and Vision Computing, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.imavis.2022.104613', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">for</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> scoring <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 48 in source list: Dan-Sebastian Bacea, Florin Oniga. "Single stage architecture for improved accuracy real-time object detection on mobile devices", Image and Vision Computing, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.imavis.2022.104613', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">the bounding box, it</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> will be due to the scale of the different sizes of the bounding box may appear to have a similar relative error, so the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 49 in source list: Haoxuan Xu, Songning Lai, Xianyang Li, Yang Yang. "Cross-domain car detection model with integrated convolutional block attention mechanism", Image and Vision Computing, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.imavis.2023.104834', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">Generalized Intersection over Union (GIoU</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) loss and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 49 in source list: Haoxuan Xu, Songning Lai, Xianyang Li, Yang Yang. "Cross-domain car detection model with integrated convolutional block attention mechanism", Image and Vision Computing, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.imavis.2023.104834', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 𝐾1 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 49 in source list: Haoxuan Xu, Songning Lai, Xianyang Li, Yang Yang. "Cross-domain car detection model with integrated convolutional block attention mechanism", Image and Vision Computing, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.imavis.2023.104834', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">loss</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> is taken as <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 49 in source list: Haoxuan Xu, Songning Lai, Xianyang Li, Yang Yang. "Cross-domain car detection model with integrated convolutional block attention mechanism", Image and Vision Computing, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.imavis.2023.104834', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> bounding box scoring 𝐾box as in equations (8-9): 𝐾box(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=96.1740538856962&svr=6&lang=zh_hans&oid=oid:1:2313591160&n=1&perc=1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">𝑏𝑖, 𝑏𝜎̂(𝑖</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">)) = 𝜆iou𝐾iou(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=96.1740538856962&svr=6&lang=zh_hans&oid=oid:1:2313591160&n=1&perc=1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">𝑏𝑖, 𝑏𝜎̂(𝑖</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">)) + 𝜆𝐿1‖<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=96.1740538856962&svr=6&lang=zh_hans&oid=oid:1:2313591160&n=1&perc=1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">𝑏𝑖 − 𝑏𝜎̂(𝑖)‖1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> (8) |<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=96.1740538856962&svr=6&lang=zh_hans&oid=oid:1:2313591160&n=1&perc=1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">𝑏𝜎(𝑖</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) ∩ 𝑏𝑖̂| |<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=96.1740538856962&svr=6&lang=zh_hans&oid=oid:1:2313591160&n=1&perc=1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">𝐵(𝑏𝜎(𝑖), 𝑏𝑖̂) \𝑏𝜎(𝑖) ∪ 𝑏𝑖̂</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">| 𝐾iou(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=96.1740538856962&svr=6&lang=zh_hans&oid=oid:1:2313591160&n=1&perc=1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">𝑏𝜎(𝑖), 𝑏𝑖̂</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) = 1 − ( − |<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=96.1740538856962&svr=6&lang=zh_hans&oid=oid:1:2313591160&n=1&perc=1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">𝑏𝜎(𝑖</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) ∪ 𝑏𝑖̂| |<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=96.1740538856962&svr=6&lang=zh_hans&oid=oid:1:2313591160&n=1&perc=1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">𝐵(𝑏𝜎(𝑖</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">), 𝑏𝑖̂)| ) (9) <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: Submitted to Korea Advanced Institute of Science and Technology on 2022-05-10"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=96.1740538856962&svr=6&lang=zh_hans&oid=oid:1:2313591160&n=1&perc=1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">where</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 𝜆iou,𝜆𝐿1 is a parameter that adjusts the proportion of GIoU and 𝐾1 paradigm loss,. |𝑏𝜎(𝑖) ∩ 𝑏𝑖̂| denotes the area <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 41 in source list: Yongfang Xie, Weitao Hu, Shiwen Xie, Lei He. "Surface Defect Detection Algorithm Based on Feature-Enhanced YOLO", Cognitive Computation, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s12559-022-10061-z', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">of the intersection between the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> true bounding <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 41 in source list: Yongfang Xie, Weitao Hu, Shiwen Xie, Lei He. "Surface Defect Detection Algorithm Based on Feature-Enhanced YOLO", Cognitive Computation, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s12559-022-10061-z', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">box</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 𝑏𝜎(𝑖) <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 41 in source list: Yongfang Xie, Weitao Hu, Shiwen Xie, Lei He. "Surface Defect Detection Algorithm Based on Feature-Enhanced YOLO", Cognitive Computation, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s12559-022-10061-z', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">and the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> predicted bounding <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 41 in source list: Yongfang Xie, Weitao Hu, Shiwen Xie, Lei He. "Surface Defect Detection Algorithm Based on Feature-Enhanced YOLO", Cognitive Computation, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s12559-022-10061-z', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">box</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 𝑏𝑖̂, |𝑏𝜎(𝑖) ∪ <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 63 in source list: Arnob Chatterjee, Soumik Chatterjee, Barbara Smith, James E. Cresswell, Parthiba Basu. "Predicted thresholds for natural vegetation cover to safeguard pollinator services in agricultural landscapes", Agriculture, Ecosystems & Environment, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.agee.2019.106785', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">𝑏𝑖̂| denotes the area of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> concatenated set; |𝐵(𝑏𝜎(𝑖), 𝑏𝑖̂)| <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 50 in source list: Jiajun Zhu, Man Cheng, Qifan Wang, Hongbo Yuan, Zhenjiang Cai. "Grape Leaf Black Rot Detection Based on Super-Resolution Image Enhancement and Deep Learning", Frontiers in Plant Science, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3389/fpls.2021.695749', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">denotes the area of the minimum closure region</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, |<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 27 in source list: Diego A. Velazquez, Josep M. Gonfaus, Pau Rodriguez, F. Xavier Roca, Seiichi Ozawa, Jordi Gonzalez. "Logo Detection with No Priors", IEEE Access, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ACCESS.2021.3101297', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">𝐵(𝑏𝜎(𝑖), 𝑏𝑖̂) ∖ 𝑏𝜎(𝑖) ∪ 𝑏𝑖̂</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">| denotes <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 35 in source list: https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/50373/Zhang_washington_0250E_25465.pdf?isAllowed=y&sequence=1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=903035770&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">the area of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> minimum closure <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 35 in source list: https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/50373/Zhang_washington_0250E_25465.pdf?isAllowed=y&sequence=1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=903035770&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">region and the area of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> difference set of the concatenated set of 𝑏𝜎(𝑖) and 𝑏𝑖̂. (c) Segmentation loss The Segmentation mask <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 46 in source list: https://s3-eu-west-1.amazonaws.com/pstorage-techrxiv-6044451694/28948746/jbhi20210722.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA3OGA3B5WJZPQBW6Z/20230204/eu-west-1/s3/aws4_request&X-Amz-Date=20230204T191748Z&X-Amz-Expires=10&X-Amz-Signature=838d9ffe95c4ff0f223be12dfa9879b1b6550cd8845844ea3d1525b853915724&X-Amz-SignedHeaders=host"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=2241423501&n=3800&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">can be</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> viewed <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 46 in source list: https://s3-eu-west-1.amazonaws.com/pstorage-techrxiv-6044451694/28948746/jbhi20210722.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA3OGA3B5WJZPQBW6Z/20230204/eu-west-1/s3/aws4_request&X-Amz-Date=20230204T191748Z&X-Amz-Expires=10&X-Amz-Signature=838d9ffe95c4ff0f223be12dfa9879b1b6550cd8845844ea3d1525b853915724&X-Amz-SignedHeaders=host"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=2241423501&n=3800&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">as a pixel-level classification problem</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> for binary classification, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 46 in source list: https://s3-eu-west-1.amazonaws.com/pstorage-techrxiv-6044451694/28948746/jbhi20210722.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA3OGA3B5WJZPQBW6Z/20230204/eu-west-1/s3/aws4_request&X-Amz-Date=20230204T191748Z&X-Amz-Expires=10&X-Amz-Signature=838d9ffe95c4ff0f223be12dfa9879b1b6550cd8845844ea3d1525b853915724&X-Amz-SignedHeaders=host"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=2241423501&n=3800&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">which</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> consists of a Dice loss 𝐾dice and a Focal loss 𝐾focal, i.e., 𝐾seg = 𝜆dice𝐾𝑏𝑖𝑏𝑏 + 𝜆focal𝐾focal, as in Eq. (10-13): 𝐾dice(𝒎, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 44 in source list: Samaniego-Riera, Franklin Eduardo, Sanchís Saez, Javier, Garcia-Nieto, Sergio, Simarro Fernández, Raúl. "Recursive Rewarding Modified Adaptive Cell Decomposition (RR-MACD): A Dynamic Path Planning Algorithm for UAVs", 'MDPI AG', 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1447128665&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">𝒎̂) =1 − |𝒎</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">| + |Sigmoid(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 44 in source list: Samaniego-Riera, Franklin Eduardo, Sanchís Saez, Javier, Garcia-Nieto, Sergio, Simarro Fernández, Raúl. "Recursive Rewarding Modified Adaptive Cell Decomposition (RR-MACD): A Dynamic Path Planning Algorithm for UAVs", 'MDPI AG', 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1447128665&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">𝒎̂)| + 1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 2|<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 44 in source list: Samaniego-Riera, Franklin Eduardo, Sanchís Saez, Javier, Garcia-Nieto, Sergio, Simarro Fernández, Raúl. "Recursive Rewarding Modified Adaptive Cell Decomposition (RR-MACD): A Dynamic Path Planning Algorithm for UAVs", 'MDPI AG', 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1447128665&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">𝒎</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ⊙ Sigmoid(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 44 in source list: Samaniego-Riera, Franklin Eduardo, Sanchís Saez, Javier, Garcia-Nieto, Sergio, Simarro Fernández, Raúl. "Recursive Rewarding Modified Adaptive Cell Decomposition (RR-MACD): A Dynamic Path Planning Algorithm for UAVs", 'MDPI AG', 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1447128665&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">𝒎̂)| + 1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> (10) 𝐾focal(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 44 in source list: Samaniego-Riera, Franklin Eduardo, Sanchís Saez, Javier, Garcia-Nieto, Sergio, Simarro Fernández, Raúl. "Recursive Rewarding Modified Adaptive Cell Decomposition (RR-MACD): A Dynamic Path Planning Algorithm for UAVs", 'MDPI AG', 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1447128665&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">𝒎</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, 𝒎̂) = ⋅ Mean(𝛼𝑡(1 − 𝑜𝑡)𝛾 ⊙ BCE(𝒎, 𝒎̂)) 1 𝐾 (11) 𝑜𝑡 = Sigmoid(𝒎̂) ⊙ 𝒎 + (1 − Sigmoid(𝒎̂)) ⋅ (1 − 𝒎) (12) 𝛼𝑡 = 𝛼 ⊙ 𝒎 + (1 − 𝛼)(1 − 𝒎) (13) where 𝜆dice,𝜆focal ditto are the training hyperparameters; 𝒎, 𝒎̂ denote the individual true mask and predicted mask probabilities of the match. BCE refers to <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: https://d-nb.info/129739903X/34"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=3824262591&n=3802&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">Binary Cross Entropy</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: https://d-nb.info/129739903X/34"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=3824262591&n=3802&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">loss</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> function <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: https://d-nb.info/129739903X/34"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=3824262591&n=3802&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">used for binary classification</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. (d) Classification loss Classification loss consists of counting loss and category loss: since DETR directly generates object prediction frames and categorization, there will be 𝜙 in the prediction, and the error of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 21 in source list: https://ebin.pub/urban-intelligence-and-applications-proceedings-of-icuia-2019-1st-ed-9783030450984-9783030450991.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1089547233&n=3802&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">counting the number of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> non-𝜙 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 21 in source list: https://ebin.pub/urban-intelligence-and-applications-proceedings-of-icuia-2019-1st-ed-9783030450984-9783030450991.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=1089547233&n=3802&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">and the number of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> GTs will be used as the counting loss for categorization scoring; secondly, the category loss is based on the calculation of cross entropy of matched prediction frame categories and GT categories, i.e., back to the calculation of the loss of the multiclassification problem, and the whole categorization loss 𝐾cls consists <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-16446-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> counting <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-16446-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">loss and the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> categorization <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-16446-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">loss as</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> shown <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: "Medical Image Computing and Computer Assisted Intervention – MICCAI 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-16446-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">in Eq</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. (14). 𝐿 𝐾cls =𝜆𝐿1||𝐾𝜎 ̂− 𝐾𝜎||1 + (−∑𝑏𝜎(𝑖)log(𝑏𝑖̂)) (14) 𝑖 D. Evaluation metric In traditional image segmentation tasks, Average Pixel Accuracy is used as an evaluation metric, which only works on the pixel level; while in instance segmentation tasks, Average Precision (AP) <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 39 in source list: Jie Li, Zuling Wang. "Visual Transformer for Image Splicing Localization", 2023 4th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICBASE59196.2023.10303114', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">and IoU are used as evaluation metrics</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, which only work <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 39 in source list: Jie Li, Zuling Wang. "Visual Transformer for Image Splicing Localization", 2023 4th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICBASE59196.2023.10303114', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">on the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> instance level; therefore, metrics metrics for panorama segmentation have three major requirements:(a) Completeness: metrics should be handled in a unified way to (a) Completeness: metrics should be handled in a unified way for Stuff and Things classes, capturing all aspects; (b) Interpretability: evaluation metrics should be clearer and more interpretable compared to the model architecture; and (c) Simplicity: metrics are easy to define and implement, and can be computed efficiently to achieve fast evaluation. Therefore, Panoptic Quality (PG) is derived as an evaluation metric for panoptic segmentation by unifying all classes of Things and Stuff, as in equation (15). ∑(𝑜, 𝑏)∈TP 𝐼𝑜𝑇(𝑜, 𝑔) ∑(𝑜, 𝑏)∈𝑇𝑃 IoU(𝑜, 𝑔) PQ = 1 1 = ⏟ |TP| + 2|TP| 2⏟|TP| + |FP| + |FN| (15) |TP| + 2 |FP| + <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 28 in source list: Wouter A. J. Van den Broeck, Toon Goedemé. "Combining Deep Semantic Edge and Object Segmentation for Large-Scale Roof-Part Polygon Extraction from Ultrahigh-Resolution Aerial Imagery", Remote Sensing, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3390/rs14194722', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">2 |FN| Segmentation Quality (SQ) Recognition Quality (RQ) Where p</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> denotes <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 28 in source list: Wouter A. J. Van den Broeck, Toon Goedemé. "Combining Deep Semantic Edge and Object Segmentation for Large-Scale Roof-Part Polygon Extraction from Ultrahigh-Resolution Aerial Imagery", Remote Sensing, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3390/rs14194722', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">the predicted</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> single object; 𝑔 denotes the true labeling that matches 𝑜; <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 37 in source list: https://WWW.MDPI.COM/2072-666X/14/2/442"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=512583331&n=3805&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">TP (True Positives), FP (False Positives), and FN (False Negatives</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) denote the predicted and GT-matched segmentation segments, the mismatched predicted segments, and the set of mismatched GTs. SQ is the introduction of TP to the IoU, the The segmentation quality of Things and Stuff are combined to make the score; RQ is similar to the F1 score of the binary classification task, and its nature is the same as <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://ebin.pub/computer-vision-eccv-2022-workshops-tel-aviv-israel-october-2327-2022-proceedings-part-i-3031250559-9783031250552.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=2683931900&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">the F1 score, which</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> combines <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://ebin.pub/computer-vision-eccv-2022-workshops-tel-aviv-israel-october-2327-2022-proceedings-part-i-3031250559-9783031250552.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.1740538856962&svr=6&lang=zh_hans&sid=2683931900&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> evaluation indexes <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://ebin.pub/computer-vision-eccv-2022-workshops-tel-aviv-israel-october-2327-2022-proceedings-part-i-3031250559-9783031250552.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=2683931900&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">of precision and recall</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. III. Experiments A. Dataset MS COCO (Microsoft Common Object in Context) dataset is a large dataset first released by Microsoft in 2014, the team collects about 330,000 images of daily life scenes and manually annotates 1.5 million object instances on 200,000 of them, which contains 80 classes of target detection and segmentation, 91 classes of pixel level image segmentation, human body key-points, and other tasks of the labeling content. The panoptic segmentation annotation was added to the second version of Training and Validation proposed in 2017. B. Results Model training and validation were done <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: http://export.arxiv.org/pdf/2003.13198"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=1651709656&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">on a Linux</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> device <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: http://export.arxiv.org/pdf/2003.13198"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=1651709656&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">with Intel(R) Xeon(R</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) Gold 6258R <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: http://export.arxiv.org/pdf/2003.13198"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=1651709656&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">CPU @ 2</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">.70GHz processor <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: http://export.arxiv.org/pdf/2003.13198"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=1651709656&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 2 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: http://export.arxiv.org/pdf/2003.13198"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=1651709656&n=3791&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">NVIDIA</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> GeForce RTX 3080 Ti 12GB graphics cards. The training process setup hyperparameters <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: https://export.arxiv.org/pdf/2210.05958"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=2996317078&n=3805&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">are shown in Table 1. Table 1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Hyperparameters for <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: https://export.arxiv.org/pdf/2210.05958"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=2996317078&n=3805&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">training</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Hyperparameter Value Hyperparameter Value 𝐾enc 6 Dropout 0.1 𝐾dec 6 𝜆cls 1 𝐾 100 𝜆seg 1 𝐾 8 𝜆box 5 𝑏 256 𝜆dice 1 𝑏𝑏𝑏 2048 𝜆focal 1 𝜆iou 2 𝜆𝐿1 1 Here the trained completed weights of the DETR base architecture are introduced and only the mask head is trained with model weights. AdamW is taken as the optimizer and trained for 100 epochs, the experimental results obtained are <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: "Neural Information Processing", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70096-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">compared with the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> base of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: "Neural Information Processing", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70096-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">other models</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> as <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: "Neural Information Processing", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70096-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">shown in Table</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 2. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: "Neural Information Processing", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70096-0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">Table</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 2. Experimental results PQ comparison. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: Dai, Zhigang, Cai, Bolun, Lin, Yugeng, Chen, Junying. "UP-DETR: Unsupervised Pre-training for Object Detection with  Transformers", 2021"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=1484602531&n=3793&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">Model PQ SQ RQ PQth SQth RQth PQst SQst RQst</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> PanopticFPN++ 42.4 79.3 51.6 49.2 82.4 58.8 32.3 74.8 40.6 UPSnet 42.5 78.0 52.5 48.6 79.4 59.6 33.4 75.9 41.7 DETR 42.8 78.6 53.8 48.1 78.9 59.4 35.8 78.2 45.1 From the above, DETR with the addition of mask head gets good results in COCO 2017 Validation experiments, and its comprehensive strength exceeds that of other architectures dedicated to panoptic segmentation in the same period. In panoptic segmentation, the pixel segmentation accuracy for STUFF far exceeds that of other models, and is slightly lacking in THINGS. It is noted that the accuracy in RQ, i.e., for instance classification, is higher than the previous ones in both synthesis, things and stuff, which proves that the features obtained from object queries of Transformer decoder can still maintain the independence and unity of the feature information when they are used in both the shared parameter FFN and the mask head, and the generated instance segmentation maps have a strong correlation with the instance segmentation maps. The generated instance segmentation graph and the category probability have a strong correlation, and the DETR-based architecture obtains a higher quality performance while completing the task than other multi-branch task architectures that are also used for panoptic segmentation. The visualization part of the results is shown in Figure 8. Figure 8. Visualization of small part segmentation figure results. IV. Conclusions In this paper, on the basis of DETR with end-to-end object detection architecture, a new mask head with similar style to Mask R-CNN is added to accomplish the segmentation map task of converting features into panoptic segmentation format. the FPN-style multiscale fusion features taken in the mask head , the loss function is increased by dice and focal loss, both of which are optimized to some extent for the extreme <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 57 in source list: https://www.frontiersin.org/articles/10.3389/fnins.2023.1212049/full"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=4517635&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">imbalance in the number of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> stuff <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 57 in source list: https://www.frontiersin.org/articles/10.3389/fnins.2023.1212049/full"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=4517635&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> things <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 57 in source list: https://www.frontiersin.org/articles/10.3389/fnins.2023.1212049/full"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=4517635&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">samples</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> (pixel points) <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 57 in source list: https://www.frontiersin.org/articles/10.3389/fnins.2023.1212049/full"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=36.2090122430914&svr=6&lang=zh_hans&sid=4517635&n=3804&svr=6&session-id=a8a68311242c4bb5b1a7d2d84ce2877b', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the panoptic segmentation task. For the processing of the final output part, the shared-parameter FFN and the Convolutional Multi-head Attention mask head based on the MHA soft query not only handle the output of the transformer encoder and decoder properly, but also solve the problem of instance segmentation pixel loss in the previous panoptic segmentation in which instance segmentation pixel region and category classification information are not related. The above is only <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: "Computer Vision – ECCV 2020", Springer Science and Business Media LLC, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-58452-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">the parameters of the mask head</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> trained on <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: "Computer Vision – ECCV 2020", Springer Science and Business Media LLC, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-58452-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> basis of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: "Computer Vision – ECCV 2020", Springer Science and Business Media LLC, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-58452-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> DETR infrastructure, if it start training the whole architecture from 0, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 60 in source list: Haruki Fujii, Hayato Tanaka, Momoko Ikeuchi, Kazuhiro Hotta. "X-net with Different Loss Functions for Cell Image Segmentation", 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CVPRW53098.2021.00420', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">we may be able to achieve</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> higher <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 60 in source list: Haruki Fujii, Hayato Tanaka, Momoko Ikeuchi, Kazuhiro Hotta. "X-net with Different Loss Functions for Cell Image Segmentation", 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CVPRW53098.2021.00420', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">accuracy</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Reference [1] Y. Lecun, L. Bottou, Y. Bengio, et al., “Gradient-Based Learning Applied to Document Recognition”, Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-3324, 1998. [2] A. Krizhevsky, I. Sutskever, G. E. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks”, Advances in Neural Information Processing Systems, vol. 25, pp. 1106-1114, 2012. [3] K. Simonyan, A. Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, 3rd International Conference on Learning Representations (ICLR 2015), 2015. [4] K. He, X. Zhang, S. Ren, et al., “Deep Residual Learning for Image Recognition”, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016), pp. 770-778, 2016. [5] J. Long, E. Shelhamer, T. Darrell, “Fully convolutional networks for semantic segmentation”, IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015), pp. 3431-3440, 2015. [6] O. Ronneberger, P. Fischer, T. Brox, “U-Net: Convolutional Networks for Biomedical Image Segmentation”, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, vol. 9351, pp. 234-241, 2015. [7] F. Milletari, N. Navab, S. A. Ahmadi, “V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation”, Fourth International Conference on 3D Vision (3DV 2016), pp. 565- 571, 2016. [8] B. Hariharan, P. A. Arbeláez, R. B. Girshick, et al., “Simultaneous Detection and Segmentation”, Computer Vision - ECCV 2014 - 13th European Conference, vol. 8695, pp. 297-312, 2014. [9] K. He, G. Gkioxari, P. Dollár, et al., “Mask R-CNN”, IEEE International Conference on Computer Vision (ICCV 2017), pp. 2980-2988, 2017. [10]T. Y. Lin, M. Maire, S. J. Belongie, et al., “Microsoft COCO: Common Objects in Context”, Computer Vision - ECCV 2014 - 13th European Conference, pp. 740-755, 2014. [11]D. Bolya, C. Zhou, F. Xiao, et al., “YOLACT: Real-Time Instance Segmentation”, 2019 IEEE/CVF International Conference on Computer Vision (ICCV 2019), pp. 9156-9165, 2019. [12]A. Kirillov, K. He, R. B. Girshick, et al., “Panoptic Segmentation”, IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019), pp. 9404-9413, 2019. [13]A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is All You Need”, Advances in Neural Information Processing Systems: Vol. 30, pp. 5998-6008, 2017. [14]A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al., “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, 9th International Conference on Learning Representations (ICLR 2021), 2021. [15]N. Carion, F. Massa, G. Synnaeve, et al., “End-to-End Object Detection with Transformers”, Computer Vision - ECCV 2020 - 16th European Conference, vol. 12346, pp. 213-229, 2020. [16]S. Zheng, J. Lu, H. Zhao, et al., “Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers”, IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021), pp. 6881-6890, 2021. [17]B. Cheng, A. G. Schwing, A. Kirillov, “Per-Pixel Classification is Not All You Need for Semantic Segmentation”, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021 (NeurIPS 2021), pp. 17864-17875, 2021. </p></div></body></html>
