In Biometrics technology, handwritten signatures are more important personal identity verification features, and the core challenge of its verification algorithms or models lies in the high intraclass variation of signatures (natural variations of signatures of the same person) and the low interclass variation (similarity between forged signatures and real signatures), and the models and algorithms will be evaluated based on these two directions. The handwritten signature verification task, on the other hand, is categorized into offline and online verification depending on how the image data is processed. In this work, focus on the offline handwritten signature verification task with static images, which is different from online verification in that the user's signature is dynamically accepted on the digitizing device, i.e., the handwriting data is changing in real time, and all the information from the beginning to the end of the process of writing will be collected; and the offline verification is completely through the static image data in order to carry out the user's verification, which does not set any limitations on the digitizing device, and it only needs to be done through the This method does not set any limitations on the digitizing device, and only requires the author to write on a form, thus greatly reducing the workload in the data collection process.

The image data collected by offline handwritten signature needs to be pre-processed to a certain extent for model or algorithm validation, and good data pre-processing techniques will to a certain extent improve the system validation accuracy. In order to improve the quality of signature images, scholars have adopted the image processing of converting RGB images to GRAY single-channel images and smoothing pixels to remove noise \cite{40}. This processing can well reduce the unnecessary features of handwritten signature images collected on white paper, such as the value of some channel pixel points in the RGB three-channel image is 255, while the value of pixel points with handwritten handwriting is often very small. However, this processing method has the defect that the value of the pixel points in the part with handwriting features is very small while the value of the pixel points in the other blank parts is very large, so the subsequent scholars processed these images with black and white inversion, and it was verified that this processing method can get a better verification accuracy rate \cite{11}.

In the early stage of offline signature verification in the 1990s-2000s period, scholars based on geometric features to distinguish the authenticity of the signature image sample pairs \cite{31}, by manually extracting the image aspect ratio, stroke length and other global features and inflection points, curvature, and other local features for matching recognition. This approach laid the initial feature extraction template for offline signature verification, where similarity calculation is performed after manual feature extraction. In 1996, Lee et al. proposed template matching for offline signature verification, which aligns the signature trajectories by dynamic time regularization (DTW) to determine whether the signature is forged or not \cite{20}. With the development of statistical theory, two schools of work emerged in the 2000s-2010s period: feature engineering dominated by a series of feature design and classifier design. In 2011, Vargas et al. mentioned for the first time the introduction of feature engineering into feature extraction for offline signature verification by combining texture engineering (LBP, HOG) with structural features (gradient direction distribution). These feature engineering will lead to a drastic increase in the number of signature image features, thus requiring scholars to start adopting machine learning approaches in order to act as classifiers, e.g., Edson et al. proposed an offline signature verification system in 2005 using Hidden Markov Models \cite{17}, where the HMM model focuses on modeling the variation of the allowable changes in signatures, i.e., treating each pixel of the image as a word vector, in order to better capture the relationship between the strokes relationships; Malik et al. in 2013 used SVM as a classifier to accomplish handwritten signature verification with higher dimensional feature counts, SVM is one of the most commonly used classifiers for binary classification tasks among academics, and it can also deal with high feature dimensions, so most of the follow-ups have taken SVM in the classifier stage. Even though traditional machine learning methods have improved the performance of early signature verification systems, they still have the defect of relying greatly on manual feature extraction, and manually designing a large number of feature extraction steps will lead to too large a difference in the individual feature parts, which cannot ensure that the merged feature vectors can be statistically significant, and manually designing the features is difficult to capture complex patterns, such as pen pressure and local texture. Moreover, the models trained by this method have insufficient generalization ability, are poorly adapted to cross-dataset and cross-linguistic scenarios, and are extremely costly in terms of computational time \cite{28}.

With the development of computer hardware, offline handwritten signatures are gradually transitioning to deep learning. The early stage of deep learning was a development process dominated by convolutional neural networks, and in 2012 Krizhevsky A. et al. proposed AlexNet \cite{19} to win the ImageNet competition's image classification task far ahead of the second-place participant using traditional machine learning algorithms, which made deep learning represented by convolutional neural networks gradually become the mainstream method for image tasks. Subsequently, academics introduced a deeper convolutional neural network, VGG \cite{34}, which is an architecture that adds more convolutional layers on top of AlexNet to extract more channels of feature maps. However, this computation of stacked convolutional layers makes the feature map size smaller while leading to the loss of features at each stage, in order to solve this problem, in 2016 K. He et al. proposed ResNet with residual connectivity \cite{14}. A residual connection is added between the convolutional layers of each size feature map to accumulate the feature maps from the original input model layer to the output feature maps. This way in supervised learning convolutional neural network, after weight sharing convolutional kernel operation, can somewhat compensate for the original image feature information, and this residual connection becomes the mainstream way of retaining local features in subsequent deep models. The high-quality performance of convolutional neural networks on image classification tasks has led to offline handwritten signature verification to start resorting to convolutional neural networks in order to replace the earlier step of manually designing features. In 2017, L. G. Hafemann et al. used convolutional neural networks to extract signature image features for the first time for verification \cite{11}, adopting the convolutional partial operation to output feature maps as signature image features, and designing two fully-connected layers in the output stage to share the signature image features to output the writer ID and signature category. This approach combines both WI and WD tasks, satisfies both outputs and is shown to significantly improve the accuracy of offline signature verification. This approach and its reliance on model training skills, the training process and its difficulty in achieving parameter convergence results, but also provides ideas for subsequent handwritten signature verification tasks using deep learning methods. In 2017, Y. Hafemann et al. proposed that using pre-trained CNNs (e.g., VGG) to extract global and local features of signature images and feeding the features into SVMs for classification can significantly improve generalization across datasets \cite{11} with end-to-end optimization. Similarly M. Diaz et al. in 2019 trained CNNs via migration learning combined with SVM as a joint classifier to achieve low error rates on the GPDS dataset \cite{35}. This CNN+SVM approach successfully reduces the error rate of the signature verification task model to a level unmatched by traditional machine learning, which are followed by this deep learning model as a feature extractor and SVM as a joint classifier in order to perform signature verification. And according to the image size of the dataset, S. Dey et al. proposed SigNet \cite{32} in 2020, which fuses multi-scale CNN features with SVM, adopting different scale feature maps outputted from multiple convolutional layers in CNN as signature image features, and optimizing classification boundaries by combining with SVM, and experiments have proved that the multi-scale features will greatly improve the robustness of the overall model, and have a very good model across datasets. The experiments proved that multi-scale features will greatly improve the robustness of the overall model and have very good model performance across data sets. However, CNN has difficulty in capturing the global contextual information of signature images, such as the long-distance dependence between strokes, in the learning ability of signature image features.

With the development of the natural language processing field of deep learning, A. Vaswani et al. proposed the Transformer with Encoder-Decoder architecture based on the attention mechanism in 2017 \cite{36}, the attention mechanism can have the ability of global feature awareness for word vectors, which can better capture the contextual information in machine translation tasks with variable sentence lengths. This kind of machine translation task reflects its excellent global generalization ability, and the excellent global context-awareness ability of the architecture is reflected in the translation task across multiple different languages. In view of Transformer's global feature-awareness capability, A. Dosovitskiy et al. first used Transformer's attention mechanism for image classification tasks in 2021, proposing Vision Transformer (ViT) \cite{4} without convolutional operations, which chunks flattens and maps the input image as word vectors to input into Transformer Encoder for attention feature computation, and finally MLP is performed to generate image category probabilities using the feature vectors output from Encoder. This way of image feature processing method that maps the image element as token after mapping, got the recognition accuracy ahead of CNN on Transformer architecture, and reflected the powerful global generalization ability of Transformer on validation and test set, so all kinds of tasks in the field of image gradually began to take the attention mechanism for secondary processing of image features. In 2022, X. Wang et al. for the first time, ViT for offline signature verification, to solve the above CNN is difficult to capture the global context information of the signature image, the use of self-attention mechanism to model the global structure of the signature, to improve the differentiation of complex forged signatures \cite{39}, in the GPDS-960 and CEDAR \cite{30} datasets compared to the same period of time the model's Equal Error Rate decreased by 12\%. In 2023 Y. He et al. also proposed TransOSV \cite{41} based on ViT, which is an architecture that provides an end-to-end Transformer-based offline signature verification system that combines both global semantic information and local discrepancy information, and more efficiently utilizes Transformer for local and global feature modeling. It introduces a multi-layer feature combination mechanism that splices features from different modules into the final signature representation to achieve multi-angle detection of forged signatures; meanwhile, Focal Contrast Loss is designed based on the Focal Loss \cite{10} of the comparison task and the handling of the sample imbalance Double-Margin Loss \cite{25} to solve the above problems while Enhance the learning of hard samples in the public dataset of offline signature task, and also incorporate the comparison learning of different module features. And unlike the previous twin networks for offline signature verification, the part decoder is proposed, which locates and focuses on the sensitive parts of forged signatures by comparing the token-level attention of reference and query, and introduces the weight of the attention mechanism into the part features of the decoder, which enhances the model's perception of the part location of forged signatures. However, TransOSV still has defects, it adopts a multi-layer feature combination mechanism to detect signature image pairs of features from different perspectives, but it does not have a strong generalization ability in the case of different signature image scales, and the decoder part does not have a good abstract expression ability for token-level features.

In summary, this work will add backbone to extract multi-scale feature maps on the basis of TransOSV architecture, and adopt FPN Fusion to fuse multi-scale features in order to enhance the learning of multi-scale features by the model architecture. At the same time, in order to better abstract expression of multi-scale fusion features in the subsequent modules, the Conv-Module is designed with downsampling structure, and the input mapping as well as the multi-head adjustment of cross-attention are added in the part decoder, which enhances the model's ability of abstract expression of image features, and is able to strengthen the attention to the forged signatures after the learning of global contextual attention features in the Encoder localized sensitive regions. On the WD task, a global average pooling layer \cite{22} will be provided for signature category prediction as per the traditional image classification task due to the adoption of a large number of convolutional output feature maps and a multi-layer feature combination mechanism.
